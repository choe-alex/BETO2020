{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from gensim.models import Word2Vec as wv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import math\n",
    "#import PhysicallyInformedLossFunction as PhysLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary from Carbon corpus and Word2Vec model trained on all abstracts\n",
    "#Opening contents of Word2Vec model1\n",
    "\n",
    "data = '/Users/alexchoe/Desktop/Capstone/m2py-master/data/all_abstracts_model/'\n",
    "os.chdir(data)\n",
    "model1 = wv.load('all_abstract_model.model')\n",
    "vocabulary1 = list(model1.wv.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/alexchoe/Desktop/Capstone/BETO2020-master/data/carbon/'\n",
    "os.chdir(data)\n",
    "data_df = pd.read_excel('Carbon_SynAntList_Full_Refined.xlsx', skip_rows=1, nrows=2000, index_col=0)\n",
    "data_df = data_df.rename(columns = {'Unnamed: 1':'word 1', 'Unnamed: 2':'word 2','Unnamed: 3':'relationship', 'Unnamed: 4': 'label'})\n",
    "\n",
    "#Adding columns for the syn and ant score labeling as well as indices\n",
    "#replacing all NaN values with 0\n",
    "data_df['syn score'] = np.nan\n",
    "data_df['ant score'] = np.nan\n",
    "data_df['word 1 index'] = np.nan\n",
    "data_df['word 2 index'] = np.nan\n",
    "data_df = data_df.fillna(0)\n",
    "data_df = data_df[1:]\n",
    "\n",
    "#finding which words are in the pd but not in vocabulary1\n",
    "list1 = list(data_df['word 1'])\n",
    "list2 = list(data_df['word 2'])\n",
    "missing = list((set(list1).difference(vocabulary1))) + list((set(list2).difference(vocabulary1)))\n",
    "\n",
    "#keeping only the rows in the pd that have words in vocabulary1\n",
    "data_df = data_df[~data_df['word 1'].isin(missing)]\n",
    "data_df = data_df[~data_df['word 2'].isin(missing)]\n",
    "\n",
    "#reseting indeces after mask\n",
    "data_df.reset_index(inplace = True)\n",
    "\n",
    "#creating list of individual words that are both in vocabulary1 and excel sheet \n",
    "#dict.fromkeys() ensuring there are no duplicates\n",
    "common = list(set(list1)&set(vocabulary1))+list(set(list2)&set(vocabulary1))\n",
    "common = list(dict.fromkeys(common))\n",
    "common = sorted(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for demonstration\n",
    "#creating list of weights to be embedded as pre_trained()\n",
    "#list of weights -> tensor to be fed in pre_trained() function\n",
    "for i in range(len(common)):\n",
    "    common[i] = model1.wv.__getitem__(common[i]).tolist()\n",
    "\n",
    "weight = torch.tensor(common)\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for demonstration, all embedded words' Tensors\n",
    "z = embedding(torch.LongTensor(input))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for demonstration, finding the Tensors for the words with indices 1 and 2\n",
    "input = (1,2)\n",
    "x, y = embedding(torch.LongTensor(input))[0], embedding(torch.LongTensor(input))[1]\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexchoe/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#adding the index of the words as they are positioned in the list where \"common\" is the list or individual words\n",
    "#creating data set with the w2v weights instead of strings\n",
    "#adding syn scores and ant scores for good syns and ants\n",
    "#anything not \"good\" has a score of 0\n",
    "\n",
    "for i in range(len(data_df)): \n",
    "    \n",
    "    data_df['word 1 index'].iloc[i] = common.index(data_df['word 1'].iloc[i])\n",
    "    data_df['word 2 index'].iloc[i] = common.index(data_df['word 2'].iloc[i])\n",
    "    \n",
    "    data_df['word 1'].iloc[i] = model1.wv.__getitem__(str(data_df['word 1'].iloc[i])).tolist()\n",
    "    data_df['word 2'].iloc[i] = model1.wv.__getitem__(str(data_df['word 2'].iloc[i])).tolist()\n",
    "    \n",
    "    if data_df['relationship'].iloc[i] == 'syn' and data_df['label'].iloc[i] == 1:\n",
    "        data_df['syn score'].iloc[i] = 1\n",
    "        data_df['ant score'].iloc[i] = -1\n",
    "       \n",
    "    elif data_df['relationship'].iloc[i] == 'ant' and data_df['label'].iloc[i] == 1:\n",
    "        data_df['syn score'].iloc[i] = -1 \n",
    "        data_df ['ant score'].iloc[i] = 1\n",
    "        \n",
    "    else:\n",
    "        data_df['syn score'].iloc[i] = 0  \n",
    "        data_df['ant score'].iloc[i] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_json('Phase_I_DATA.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_json('Phase_I_DATA.json', dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting our data into train and test dataframes\n",
    "\n",
    "X = data_df[['word 1 index', 'word 2 index']] #indices that will be used to find corresponding \"feature\" weights\n",
    "Y = data_df[['syn score', 'ant score']] #what will ultimately be predicted\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "w1_train = x_train['word 1 index']\n",
    "w1_test = x_test['word 1 index']\n",
    "w2_train = x_train['word 2 index']\n",
    "w2_test = x_test['word 2 index']\n",
    "ss_train = y_train['syn score']\n",
    "ss_test = y_test['syn score']\n",
    "as_train = y_train['ant score']\n",
    "as_test = y_test['ant score']\n",
    "\n",
    "train_data = {'word 1 index': w1_train, 'word 2 index': w2_train, 'syn score': ss_train, 'ant score': as_train}\n",
    "test_data = {'word 1 index': w1_test, 'word 2 index': w2_test, 'syn score': ss_test, 'ant score': as_test}\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_json('Phase_I_Train.json')\n",
    "test_df.to_json('Phase_I_Test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "data_test = data_df\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "test_list = list(zip(data_test['word 1 index'], data_test['word 2 index']))\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration\n",
    "test_list_tensor = torch.tensor(test_list)\n",
    "test_list_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "learning_rate = 0.008\n",
    "\n",
    "# Device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_Train_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data = pd.read_json('Phase_I_Train.json', dtype = np.float32)\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "        data_x = list(zip(data['word 1 index'], data['word 2 index'])) #creating a list of tuples where [w1,w2] and [ss, as]\n",
    "        data_y = list(zip(data['syn score'], data['ant score']))\n",
    "            \n",
    "        #split into x_data our features and y_data our targets\n",
    "        self.x_data = torch.tensor(data_x)\n",
    "        self.y_data = torch.tensor(data_y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = Phase_I_Train_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_Test_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data = pd.read_json('Phase_I_Test.json', dtype = np.float32)\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "        data_x = list(zip(data['word 1 index'], data['word 2 index'])) #creating a list of tuples where [w1,w2] and [ss, as]\n",
    "        data_y = list(zip(data['syn score'], data['ant score']))\n",
    "            \n",
    "        #split into x_data our features and y_data our targets\n",
    "        self.x_data = torch.tensor(data_x)\n",
    "        self.y_data = torch.tensor(data_y)\n",
    "\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = Phase_I_Test_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader cell here\n",
    "training_data_set = torch.utils.data.DataLoader(dataset = Phase_I_Train_Dataset(), batch_size = batch_size, shuffle = True)\n",
    "testing_data_set = torch.utils.data.DataLoader(dataset = Phase_I_Test_Dataset(), batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Pre_Trained_Weights(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the pre-training of the Phase_I_NN neural network weights using a list of words from which\n",
    "    a list of weights can be obtained. It is then converted that can then be embedded using the from_pretrained() \n",
    "    function into the NN model\n",
    "    \"\"\"\n",
    "    def __init__(self, words, model):\n",
    "        super(Embedding_Pre_Trained_Weights, self).__init__()\n",
    "    \n",
    "        for i in range(len(words)):\n",
    "            words[i] = model.wv.__getitem__(words[i]).tolist()\n",
    "    \n",
    "        weight = torch.tensor(words)\n",
    "    \n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "    \n",
    "    def forward(self, index):\n",
    "        \n",
    "        index_vector = self.embedding(torch.LongTensor(index))\n",
    "        \n",
    "        return index_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_NN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the first of two neural networks to be used to determine synonymy, antonymy or irrelevance.\n",
    "    Using w2v pre-trained embeddings that are then embedded into our NN using the nn.Embedding layer we are able to\n",
    "    obtain the encoded embeddings of two words (pushed as a tuple) in synonym and antonym subspaces. These encodings\n",
    "    are then used to calculate the synonymy and antonymy score of those two words.\n",
    "    This mimics the Distiller method described by Asif Ali et al.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dims, out_dims, common, model1):\n",
    "        super(Phase_I_NN, self).__init__()\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedded = Embedding_Pre_Trained_Weights(common, model1)\n",
    "        \n",
    "        #hidden layers\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "        nn.Linear(50, 100), #expand\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100, 300),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.S_branch = nn.Sequential( #synonym subspace branch\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300,100), #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.A_branch = nn.Sequential(#need some activation function after each Linear function. Softmax is the NLP convention.\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300, 100), #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        #other option is to define activation function here i.e. self.Softplus = torch.nn.Softplus() and use it in the forward pass\n",
    "        \n",
    "        \n",
    "    def forward(self, index_tuple):\n",
    "        \n",
    "        em_1, em_2 = self.embedded(index_tuple)[0], self.embedded(index_tuple)[1]\n",
    "        \n",
    "        #pass through hidden layers. For each linear layer in the hidden/branches, use the activation function to push\n",
    "        out_w1 = self.hidden_layers(em_1) \n",
    "        out_w2 = self.hidden_layers(em_2)\n",
    "        \n",
    "        #pass each embedded data through each branch to be situated in subspaces\n",
    "        S1_out = self.S_branch(out_w1)\n",
    "        S2_out = self.S_branch(out_w2)\n",
    "        A1_out = self.A_branch(out_w1)\n",
    "        A2_out = self.A_branch(out_w2)\n",
    "        \n",
    "        #Need to find a way to collect encoder embeddings as well as their scoring\n",
    "        \n",
    "        synonymy_score = F.cosine_similarity(S1_out.view(1,-1),S2_out.view(1,-1), dim=0) #do these outside of the NN class\n",
    "        antonymy_score = torch.max(F.cosine_similarity(A1_out.view(1,-1),S2_out.view(1,-1),dim=0), F.cosine_similarity(A2_out.view(1,-1),S1_out.view(1,-1),dim=0))\n",
    "                              \n",
    "        #return synonymy_score, antonymy_score #the encoders in each subspace\n",
    "        \n",
    "        return S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Synonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the synonym subspace. A lower and higher bound for synonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"  \n",
    "         #will need to include S score here\n",
    "    def __init__ (self):\n",
    "        super(Loss_Synonymy, self).__init__()\n",
    "        \n",
    "    def forward(self, S1_out, S2_out, synonymy_score):\n",
    " \n",
    "        result_list = torch.zeros(S1_out.size(0))\n",
    "        element_count = 0\n",
    "        \n",
    "        error_1 = torch.zeros(1,1)\n",
    "        error_2 = torch.zeros(1,1)\n",
    "            \n",
    "        for x, a, b in zip(synonymy_score, S1_out, S2_out): #x=synscore, a=S1_out, b=S2_out\n",
    "            \n",
    "            #print(labels)\n",
    "            \n",
    "            if torch.ge(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.neg(torch.tanh(torch.dist(a, b, 2))))) #assumed Euclidean Distance\n",
    "                error_1 = torch.add(error_1,error)\n",
    "                \n",
    "            elif torch.lt(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.tanh(torch.dist(a, b, 2))))\n",
    "                error_2 = torch.add(error_2,error)\n",
    "                \n",
    "             \n",
    "        #may need to collect error_1 and error_2 independently and add them batch by batch\n",
    "        \n",
    "        error_total = torch.add(error_1, error_2)\n",
    "        \n",
    "        result_list[element_count] = error_total\n",
    "        element_count += 1\n",
    "        \n",
    "        result = result_list.mean()\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Antonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the antonym subspace. A lower and higher bound for antonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Loss_Antonymy, self).__init__()\n",
    "       \n",
    "    def forward(self, S2_out, A1_out, antonymy_score):\n",
    "  \n",
    "        result_list = torch.zeros(S2_out.size(0))\n",
    "        element_count = 0\n",
    "    \n",
    "        error_1 = torch.zeros(1,1)\n",
    "        error_2 = torch.zeros(1,1)\n",
    "        \n",
    "        for x, a, b in zip(antonymy_score, A1_out, S2_out): #x=antscore, a=A1_out, b=S2_out (to ensure trans-transitivity)\n",
    "            \n",
    "            if torch.ge(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.neg(torch.tanh(torch.dist(a, b, 2)))))\n",
    "                error_1 = torch.add(error_1,error)\n",
    "                \n",
    "            elif torch.lt(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.tanh(torch.dist(a, b, 2))))\n",
    "                error_2 = torch.add(error_2, error)\n",
    "                 \n",
    "        error_total = torch.add(error_1, error_2)\n",
    "        \n",
    "        result_list[element_count] = error_total\n",
    "        element_count += 1\n",
    "        \n",
    "        result = result_list.mean()\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Labels(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is the last portion of the general loss function. Here the predicted synonymy and antonymy scores\n",
    "    are concatenated and compared to the concatenated labeled synonymy and antonymy scores\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Loss_Labels, self).__init__()\n",
    "       \n",
    "    def forward(self, synonymy_score, antonymy_score, labels):\n",
    "        \n",
    "        print(labels)\n",
    "        result_list = torch.zeros(labels.size(0))\n",
    "        element_count = 0\n",
    "        \n",
    "        for x, y in zip(torch.cat(synonymy_score, antonymy_score), torch.cat(labels[0], labels[1])):\n",
    "            \n",
    "            error = F.arg_max(torch.nn.Softmax(x), torch.nn.Softmax(y))\n",
    "            \n",
    "            result_list[element_count] = error\n",
    "            element_count += 1\n",
    "            result = result_list.mean\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_I_train_model(model, training_data_set, optimizer):\n",
    "    train_losses = []\n",
    "    syn_train_losses = []\n",
    "    ant_train_losses = []\n",
    "    label_train_losses = []\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "    syn_train_epoch_loss = []\n",
    "    ant_train_epoch_loss = []\n",
    "    label_train_epoch_loss = []\n",
    "    \n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    syn_criterion = Loss_Synonymy()\n",
    "    ant_criterion = Loss_Antonymy()\n",
    "    label_criterion = Loss_Labels()\n",
    "    \n",
    "    for i, data in enumerate(training_data_set,0):\n",
    "        \n",
    "        features, labels = data\n",
    "        \n",
    "        #have been encountering an issue where the data is not a double() but a float()\n",
    "        features, labels = np.double(features), np.double(labels)\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score = model(features) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        syn_loss = syn_criterion(S1_out, S2_out, synonymy_score)\n",
    "        ant_loss = ant_criterion(S2_out, A1_out, antonymy_score)\n",
    "        label_loss = label_criterion(synonymy_score, antonymy_score, labels)\n",
    "        total_loss = syn_loss + ant_loss + label_loss #need to create a total_loss_criterion\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([syn_loss, ant_loss, label_loss])\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        syn_train_losses.append(syn_loss.item())\n",
    "        ant_train_losses.append(ant_loss.item())\n",
    "        label_train_losses.append(label_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    syn_train_epoch_loss.append(sum(syn_train_losses)/train_total)\n",
    "    ant_train_epoch_loss.append(sum(ant_train_losses)/train_total)\n",
    "    label_train_epoch_loss.append(sum(label_train_losses)/train_total)\n",
    "    \n",
    "    return train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss, label_train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_I_eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    syn_criterion = Loss_Synonymy() \n",
    "    ant_criterion = Loss_Antonymy()\n",
    "    label_criterion = Loss_Labels()\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        syn_test_losses = []\n",
    "        ant_test_losses = []\n",
    "        label_test_losses = []\n",
    "        \n",
    "        syn_test_acc_list = []\n",
    "        ant_test_acc_list = []\n",
    "        label_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        #for inputs, labels in testing_data_set:\n",
    "        #similar change to the train_model portion due to the nature of our data\n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        for i, data in enumerate(testing_data_set,0):\n",
    "        \n",
    "            inputs, labels = data\n",
    "        \n",
    "            #have been encountering an issue where the data is not a double() but a float()\n",
    "            inputs, labels = np.double(inputs), np.double(labels)\n",
    "            \n",
    "            S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score = model(inputs)\n",
    "\n",
    "            # calculate loss per batch of testing data\n",
    "            syn_test_loss = syn_criterion(S1_out, S2_out, synonymy_score)\n",
    "            ant_test_loss = ant_criterion(S2_out, A1_out, antonymy_score)\n",
    "            label_test_loss = label_criterion(synonymy_score, antonymy_score, labels)\n",
    "            \n",
    "            test_loss = syn_test_loss + ant_test_loss + label_test_loss \n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            syn_test_losses.append(syn_test_loss.item())\n",
    "            ant_test_losses.append(ant_test_loss.item())\n",
    "            label_test_losses.append(label_test_loss.item())\n",
    "            test_total += 1 \n",
    "            \n",
    "            #syn_test_acc_list.append(syn_acc.item())\n",
    "            #ant_test_acc_list.append(ant_acc.item())\n",
    "            #label_test_acc_list.append(label_acc.item())\n",
    "            \n",
    "            for x, y in zip(synonymy_score, labels[0]):\n",
    "                if x == y:\n",
    "                    correct += 1\n",
    "                    element_count += 1\n",
    "\n",
    "                else:\n",
    "                    element_count += 1\n",
    "            \n",
    "            for x, y in zip(antonymy_score, labels[1]):\n",
    "                if x == y:\n",
    "                    correct += 1\n",
    "                    element_count += 1\n",
    "\n",
    "                else:\n",
    "                    element_count += 1\n",
    "            \n",
    "        syn_acc = (correct/element_count) * 100\n",
    "        syn_test_acc_list.append(syn_acc)\n",
    "        \n",
    "        ant_acc = (correct/element_count) * 100\n",
    "        ant_test_acc_list.append(ant_acc)\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        syn_test_epoch_loss = sum(syn_test_losses)/test_total\n",
    "        ant_test_epoch_loss = sum(ant_test_losses)/test_total\n",
    "        label_test_epoch_loss = sum(label_test_losses)/test_total\n",
    "        \n",
    "        syn_epoch_acc = sum(syn_test_acc_list)/test_total\n",
    "        ant_epoch_acc = sum(ant_test_acc_list)/test_total\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss is: {test_epoch_loss}\")\n",
    "        #print(f\"Epoch MAPE: Syn = {syn_epoch_acc}\")\n",
    "    \n",
    "    return test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, label_test_epoch_loss, syn_epoch_acc, ant_epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our beautiful NN model\n",
    "# takes in \n",
    "# predicts synonymy\n",
    "model = Phase_I_NN(in_dims = 50, out_dims = 2, common = common, model1 = model1).to(device)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 1. -1.]\n",
      " [-1.  1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 1. -1.]\n",
      " [ 1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-e8a88a4616ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn_train_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mant_train_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train_epoch_loss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mPhase_I_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_epoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-3fffdefcbea2>\u001b[0m in \u001b[0;36mPhase_I_train_model\u001b[0;34m(model, training_data_set, optimizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0msyn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyn_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS1_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS2_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynonymy_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mant_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mant_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS2_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantonymy_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mlabel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynonymy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantonymy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyn_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mant_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabel_loss\u001b[0m \u001b[0;31m#need to create a total_loss_criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-9df7288f4d0b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, synonymy_score, antonymy_score, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0melement_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "syn_train_epoch_losses = []\n",
    "ant_train_epoch_losses = []\n",
    "label_train_epoch_losses = []\n",
    "\n",
    "test_epoch_losses = []\n",
    "syn_test_epoch_losses = []\n",
    "ant_test_epoch_losses = []\n",
    "label_test_epoch_losses = []\n",
    "\n",
    "syn_test_epoch_accuracies = []\n",
    "ant_test_epoch_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss, label_train_epoch_loss  = Phase_I_train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "    ant_train_epoch_losses.append(ant_train_epoch_loss)\n",
    "    label_train_epoch_losses.append(label_train_epoch_loss)\n",
    "   \n",
    "    test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, label_test_epoch_loss, syn_epoch_acc, ant_epoch_acc = Phase_I_eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "    ant_test_epoch_losses.append(ant_test_epoch_loss)\n",
    "    label_test_epoch_losses.append(label_test_epoch_loss)\n",
    "    \n",
    "    syn_test_epoch_accuracies.append(syn_epoch_acc)\n",
    "    ant_test_epoch_accuracies.append(ant_epoch_acc)\n",
    "    #test_epoch_losses.append(test_epoch_loss)\n",
    "    #syn_test_epoch_losses.append(syn_test_epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_II_train_model(model, training_data_set, optimizer):\n",
    "    train_losses = []\n",
    "    syn_train_losses = []\n",
    "    ant_trian_losses\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "    syn_train_epoch_loss = []\n",
    "    ant_train_epoch_loss\n",
    "    \n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    syn_criterion = nn.MSELoss()\n",
    "    ant_criterion = \n",
    "    \n",
    "    #for features, labels in training_data_set: \n",
    "    \n",
    "    #may need to change above \"features\" portion here to accomodate for our custom dataset\n",
    "    #below is proposed alternative\n",
    "    \n",
    "    for i, data in enumerate(training_data_set,0):\n",
    "        \n",
    "        features, labels = data\n",
    "        \n",
    "        #have been encountering an issue where the data is not a double() but a float()\n",
    "        features, labels = np.double(features), np.double(labels)\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        syn_out = model(features) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        syn_loss = syn_criterion(syn_out, labels)\n",
    "        total_loss = syn_loss\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([syn_loss])\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        syn_train_losses.append(syn_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    syn_train_epoch_loss.append(sum(syn_train_losses)/train_total)\n",
    "    \n",
    "    #update progress bar\n",
    "    print(f\"Total Epoch Training Loss is: {train_epoch_loss}\")\n",
    "    \n",
    "    return train_epoch_loss, syn_train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_II_eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    syn_criterion = nn.MSELoss()\n",
    "    #accuracy = #total number of correct predictions divided by the total number of predictions\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        syn_test_losses = []\n",
    "        #syn_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        #for inputs, labels in testing_data_set:\n",
    "        #similar change to the train_model portion due to the nature of our data\n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        for i, data in enumerate(testing_data_set,0):\n",
    "        \n",
    "            inputs, labels = data\n",
    "        \n",
    "            #have been encountering an issue where the data is not a double() but a float()\n",
    "            inputs, labels = np.double(inputs), np.double(labels)\n",
    "            \n",
    "            syn_out = model(inputs)\n",
    "\n",
    "            # calculate loss per batch of testing data\n",
    "            syn_test_loss = syn_criterion(syn_out, labels)\n",
    "            \n",
    "            test_loss = syn_test_loss\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            syn_test_losses.append(syn_test_loss.item())\n",
    "            test_total += 1 \n",
    "            #syn_acc = accuracy(syn_out)\n",
    "            #syn_test_acc_list.append(syn_acc.item())\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        syn_test_epoch_loss = sum(syn_test_losses)/test_total\n",
    "        \n",
    "        #syn_epoch_acc = sum(syn_test_acc_list)/test_total\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss is: {test_epoch_loss}\")\n",
    "        #print(f\"Epoch MAPE: Syn = {syn_epoch_acc}\")\n",
    "    \n",
    "    return test_epoch_loss, syn_test_epoch_loss, #syn_epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our beautiful NN model\n",
    "# takes in \n",
    "# predicts synonymy\n",
    "model = SYN_TEST(in_dims = 50, out_dims = 2).to(device)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "syn_train_epoch_losses = []\n",
    "\n",
    "test_epoch_losses = []\n",
    "syn_test_epoch_losses = []\n",
    "\n",
    "syn_test_epoch_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, syn_train_epoch_loss  = train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "   \n",
    "    test_epoch_loss, syn_test_epoch_loss = eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "    #syn_epoch_acc\n",
    "    \n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "    \n",
    "    #pce_test_epoch_accuracies.append(pce_epoch_acc)\n",
    "    #voc_test_epoch_accuracies.append(voc_epoch_acc)\n",
    "    #jsc_test_epoch_accuracies.append(jsc_epoch_acc)\n",
    "    #ff_test_epoch_accuracies.append(ff_epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
