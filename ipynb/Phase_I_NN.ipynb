{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from gensim.models import Word2Vec as wv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary from Carbon corpus and Word2Vec model trained on all abstracts\n",
    "\n",
    "data = '/Users/alexchoe/Desktop/Capstone/m2py-master/data/all_abstracts_model/'\n",
    "os.chdir(data)\n",
    "model1 = wv.load('all_abstract_model.model') #Opening contents of Word2Vec model1\n",
    "vocabulary1 = list(model1.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/alexchoe/Desktop/Capstone/BETO2020-master/data/carbon/'\n",
    "os.chdir(data)\n",
    "data_df = pd.read_excel('CombinedSynAntList.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding columns for the syn and ant score labeling as well as indices\n",
    "#replacing all NaN values with 0\n",
    "data_df['syn score'] = np.nan\n",
    "data_df['ant score'] = np.nan\n",
    "data_df['word 1 index'] = np.nan\n",
    "data_df['word 2 index'] = np.nan\n",
    "data_df = data_df.fillna(0)\n",
    "data_df = data_df[1:]\n",
    "\n",
    "#finding which words are in the pd but not in vocabulary1\n",
    "list1 = list(data_df['word 1'])\n",
    "list2 = list(data_df['word 2'])\n",
    "missing = list((set(list1).difference(vocabulary1))) + list((set(list2).difference(vocabulary1)))\n",
    "\n",
    "#keeping only the rows in the pd that have words in vocabulary1\n",
    "data_df = data_df[~data_df['word 1'].isin(missing)]\n",
    "data_df = data_df[~data_df['word 2'].isin(missing)]\n",
    "data_df = data_df[~data_df['label'].str.contains('#', na=False)]\n",
    "\n",
    "#reseting indeces after mask\n",
    "data_df.reset_index(inplace = True)\n",
    "\n",
    "#creating list of individual words that are both in vocabulary1 and excel sheet \n",
    "#dict.fromkeys() ensuring there are no duplicates\n",
    "common = list(set(list1)&set(vocabulary1))+list(set(list2)&set(vocabulary1))\n",
    "common = list(dict.fromkeys(common))\n",
    "common = sorted(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexchoe/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#adding the index of the words as they are positioned in the list where \"common\" is the list or individual words\n",
    "#creating data set with the w2v weights instead of strings\n",
    "#adding syn scores and ant scores for good syns and ants\n",
    "#anything not \"good\" has a score of 0\n",
    "\n",
    "for i in range(len(data_df)): \n",
    "    \n",
    "    data_df['word 1 index'].iloc[i] = common.index(data_df['word 1'].iloc[i])\n",
    "    data_df['word 2 index'].iloc[i] = common.index(data_df['word 2'].iloc[i])\n",
    "    \n",
    "    data_df['word 1'].iloc[i] = model1.wv.__getitem__(str(data_df['word 1'].iloc[i])).tolist()\n",
    "    data_df['word 2'].iloc[i] = model1.wv.__getitem__(str(data_df['word 2'].iloc[i])).tolist()\n",
    "    \n",
    "    if data_df['relationship'].iloc[i] == 'syn' and data_df['label'].iloc[i] == 1:\n",
    "        data_df['syn score'].iloc[i] = 1\n",
    "        data_df['ant score'].iloc[i] = -1\n",
    "       \n",
    "    elif data_df['relationship'].iloc[i] == 'ant' and data_df['label'].iloc[i] == 1:\n",
    "        data_df['syn score'].iloc[i] = -1 \n",
    "        data_df ['ant score'].iloc[i] = 1\n",
    "        \n",
    "    else:\n",
    "        data_df['syn score'].iloc[i] = 0  \n",
    "        data_df['ant score'].iloc[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting our data into train and test dataframes\n",
    "\n",
    "X = data_df[['word 1 index', 'word 2 index']] #indices that will be used to find corresponding \"feature\" weights\n",
    "Y = data_df[['syn score', 'ant score']] #what will ultimately be predicted\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "w1_train = x_train['word 1 index']\n",
    "w1_test = x_test['word 1 index']\n",
    "w2_train = x_train['word 2 index']\n",
    "w2_test = x_test['word 2 index']\n",
    "ss_train = y_train['syn score']\n",
    "ss_test = y_test['syn score']\n",
    "as_train = y_train['ant score']\n",
    "as_test = y_test['ant score']\n",
    "\n",
    "train_data = {'word 1 index': w1_train, 'word 2 index': w2_train, 'syn score': ss_train, 'ant score': as_train}\n",
    "test_data = {'word 1 index': w1_test, 'word 2 index': w2_test, 'syn score': ss_test, 'ant score': as_test}\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_json('Phase_I_Train.json')\n",
    "test_df.to_json('Phase_I_Test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "num_epochs = 50\n",
    "batch_size = 50\n",
    "learning_rate = 0.0000001\n",
    "\n",
    "# Device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_Train_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data = pd.read_json('Phase_I_Train.json', dtype = np.float32)\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "        data_x = list(zip(data['word 1 index'], data['word 2 index'])) #creating a list of tuples where [w1,w2] and [ss, as]\n",
    "        data_y = list(zip(data['syn score'], data['ant score']))\n",
    "            \n",
    "        #split into x_data our features and y_data our targets\n",
    "        self.x_data = torch.tensor(data_x)\n",
    "        self.y_data = torch.tensor(data_y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data[index], self.y_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_Test_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        data = pd.read_json('Phase_I_Test.json', dtype = np.float32)\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "        data_x = list(zip(data['word 1 index'], data['word 2 index'])) #creating a list of tuples where [w1,w2] and [ss, as]\n",
    "        data_y = list(zip(data['syn score'], data['ant score']))\n",
    "            \n",
    "        #split into x_data our features and y_data our targets\n",
    "        self.x_data = torch.tensor(data_x)\n",
    "        self.y_data = torch.tensor(data_y)\n",
    "\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data[index], self.y_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = Phase_I_Train_Dataset()\n",
    "test_data_set = Phase_I_Test_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader cell here\n",
    "training_data_set = torch.utils.data.DataLoader(dataset = train_data_set, batch_size = batch_size, shuffle = True)\n",
    "testing_data_set = torch.utils.data.DataLoader(dataset = test_data_set, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Pre_Trained_Weights(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the pre-training of the Phase_I_NN neural network weights using a list of words from which\n",
    "    a list of weights can be obtained. It is then converted that can then be embedded using the from_pretrained() \n",
    "    function into the NN model\n",
    "    \"\"\"\n",
    "    def __init__(self, words, model):\n",
    "        super(Embedding_Pre_Trained_Weights, self).__init__()\n",
    "    \n",
    "        for i in range(len(words)):\n",
    "            words[i] = model.wv.__getitem__(words[i]).tolist()\n",
    "    \n",
    "        weight = torch.tensor(words)\n",
    "    \n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "    \n",
    "    def forward(self, index):\n",
    "        \n",
    "        index_vector = self.embedding(torch.LongTensor(index))\n",
    "        \n",
    "        return index_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase_I_NN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the first of two neural networks to be used to determine synonymy, antonymy or irrelevance.\n",
    "    Using w2v pre-trained embeddings that are then embedded into our NN using the nn.Embedding layer we are able to\n",
    "    obtain the encoded embeddings of two words (pushed as a tuple) in synonym and antonym subspaces. These encodings\n",
    "    are then used to calculate the synonymy and antonymy score of those two words.\n",
    "    This mimics the Distiller method described by Asif Ali et al.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dims, out_dims, common, model1):\n",
    "        super(Phase_I_NN, self).__init__()\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedded = Embedding_Pre_Trained_Weights(common, model1)\n",
    "        \n",
    "        #hidden layers\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "        nn.Linear(50, 100), #expand\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100, 300),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.S_branch = nn.Sequential( #synonym subspace branch\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300,100), #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.A_branch = nn.Sequential(#need some activation function after each Linear function. Softmax is the NLP convention.\n",
    "        nn.Dropout(0.1), #to limit overfitting\n",
    "        nn.Linear(300, 100), #compress\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(100,50),\n",
    "        nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        #other option is to define activation function here i.e. self.Softplus = torch.nn.Softplus() and use it in the forward pass\n",
    "        \n",
    "        \n",
    "    def forward(self, index_tuple):\n",
    "        \n",
    "        em_1, em_2 = self.embedded(index_tuple)[0], self.embedded(index_tuple)[1]\n",
    "        \n",
    "        #pass through hidden layers. For each linear layer in the hidden/branches, use the activation function to push\n",
    "        out_w1 = self.hidden_layers(em_1) \n",
    "        out_w2 = self.hidden_layers(em_2)\n",
    "        \n",
    "        #pass each embedded data through each branch to be situated in subspaces\n",
    "        S1_out = self.S_branch(out_w1)\n",
    "        S2_out = self.S_branch(out_w2)\n",
    "        A1_out = self.A_branch(out_w1)\n",
    "        A2_out = self.A_branch(out_w2)\n",
    "        \n",
    "        #Need to find a way to collect encoder embeddings as well as their scoring\n",
    "        \n",
    "        synonymy_score = F.cosine_similarity(S1_out.view(1,-1),S2_out.view(1,-1), dim=1) #do these outside of the NN class\n",
    "        antonymy_score = torch.max(F.cosine_similarity(A1_out.view(1,-1),S2_out.view(1,-1),dim=1), F.cosine_similarity(A2_out.view(1,-1),S1_out.view(1,-1),dim=1))\n",
    "        \n",
    "        #print(antonymy_score.size())\n",
    "        #return synonymy_score, antonymy_score #the encoders in each subspace\n",
    "        \n",
    "        return S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Synonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the synonym subspace. A lower and higher bound for synonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"  \n",
    "         #will need to include S score here\n",
    "    def __init__ (self):\n",
    "        super(Loss_Synonymy, self).__init__()\n",
    "        \n",
    "    def forward(self, S1_out, S2_out, synonymy_score):\n",
    " \n",
    "        result_list = torch.zeros(S1_out.size(0))\n",
    "        element_count = 0\n",
    "        \n",
    "        error_1 = torch.zeros(1,1)\n",
    "        error_2 = torch.zeros(1,1)\n",
    "            \n",
    "        for x, a, b in zip(synonymy_score, S1_out, S2_out): #x=synscore, a=S1_out, b=S2_out\n",
    "            \n",
    "            #print(labels)\n",
    "            \n",
    "            if torch.ge(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.neg(torch.tanh(torch.dist(a, b, 2))))) #assumed Euclidean Distance\n",
    "                error_1 = torch.add(error_1,error)\n",
    "                \n",
    "            elif torch.lt(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.tanh(torch.dist(a, b, 2))))\n",
    "                error_2 = torch.add(error_2,error)\n",
    "                \n",
    "             \n",
    "        #may need to collect error_1 and error_2 independently and add them batch by batch\n",
    "        \n",
    "        error_total = torch.add(error_1, error_2)\n",
    "        \n",
    "        result_list[element_count] = error_total\n",
    "        element_count += 1\n",
    "        \n",
    "        result = result_list.mean()\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Antonymy(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains a loss function that uses the sum of ReLu loss to make predictions for the encoded embeddings\n",
    "    in the antonym subspace. A lower and higher bound for antonymy are to be determined. Need to better understand the\n",
    "    equation found in the Asif Ali et al. paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Loss_Antonymy, self).__init__()\n",
    "       \n",
    "    def forward(self, S2_out, A1_out, antonymy_score):\n",
    "  \n",
    "        result_list = torch.zeros(S2_out.size(0))\n",
    "        element_count = 0\n",
    "    \n",
    "        error_1 = torch.zeros(1,1)\n",
    "        error_2 = torch.zeros(1,1)\n",
    "        \n",
    "        for x, a, b in zip(antonymy_score, A1_out, S2_out): #x=antscore, a=A1_out, b=S2_out (to ensure trans-transitivity)\n",
    "            \n",
    "            if torch.ge(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.neg(torch.tanh(torch.dist(a, b, 2)))))\n",
    "                error_1 = torch.add(error_1,error)\n",
    "                \n",
    "            elif torch.lt(x, torch.tensor(0.8)) == True:\n",
    "                error = F.relu(torch.add(torch.tensor(1), torch.tanh(torch.dist(a, b, 2))))\n",
    "                error_2 = torch.add(error_2, error)\n",
    "                 \n",
    "        error_total = torch.add(error_1, error_2)\n",
    "        \n",
    "        error_total.retain_grad()\n",
    "        result_list[element_count] = error_total\n",
    "        element_count += 1\n",
    "        \n",
    "        result = result_list.mean()\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_I_train_model(model, training_data_set, optimizer):\n",
    "    train_losses = []\n",
    "    syn_train_losses = []\n",
    "    ant_train_losses = []\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "    syn_train_epoch_loss = []\n",
    "    ant_train_epoch_loss = []\n",
    "    \n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    syn_criterion = Loss_Synonymy()\n",
    "    ant_criterion = Loss_Antonymy()\n",
    "    \n",
    "    for i, data in enumerate(training_data_set,0):\n",
    "        \n",
    "        features, labels = data\n",
    "        \n",
    "        #have been encountering an issue where the data is not a double() but a float()\n",
    "        features, labels = np.double(features), np.double(labels)\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score = model(features) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        syn_loss = syn_criterion(S1_out, S2_out, synonymy_score)\n",
    "        ant_loss = ant_criterion(S2_out, A1_out, antonymy_score)\n",
    "        total_loss = syn_loss + ant_loss #need to create a total_loss_criterion\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([syn_loss, ant_loss])\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        syn_train_losses.append(syn_loss.item())\n",
    "        ant_train_losses.append(ant_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    syn_train_epoch_loss.append(sum(syn_train_losses)/train_total)\n",
    "    ant_train_epoch_loss.append(sum(ant_train_losses)/train_total)\n",
    "    \n",
    "    return train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phase_I_eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    syn_criterion = Loss_Synonymy() \n",
    "    ant_criterion = Loss_Antonymy()\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        syn_test_losses = []\n",
    "        ant_test_losses = []\n",
    "        \n",
    "        syn_test_acc_list = []\n",
    "        ant_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        #for inputs, labels in testing_data_set:\n",
    "        #similar change to the train_model portion due to the nature of our data\n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        for i, data in enumerate(testing_data_set,0):\n",
    "        \n",
    "            inputs, labels = data\n",
    "        \n",
    "            #have been encountering an issue where the data is not a double() but a float()\n",
    "            inputs, labels = np.double(inputs), np.double(labels)\n",
    "            \n",
    "            S1_out, S2_out, A1_out, A2_out, synonymy_score, antonymy_score = model(inputs)\n",
    "\n",
    "            # calculate loss per batch of testing data\n",
    "            syn_test_loss = syn_criterion(S1_out, S2_out, synonymy_score)\n",
    "            ant_test_loss = ant_criterion(S2_out, A1_out, antonymy_score)\n",
    "            \n",
    "            test_loss = syn_test_loss + ant_test_loss\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            syn_test_losses.append(syn_test_loss.item())\n",
    "            ant_test_losses.append(ant_test_loss.item())\n",
    "            test_total += 1 \n",
    "            \n",
    "            #syn_test_acc_list.append(syn_acc.item())\n",
    "            #ant_test_acc_list.append(ant_acc.item())\n",
    "            #label_test_acc_list.append(label_acc.item())\n",
    "            \n",
    "            syn_el_count = 0\n",
    "            syn_correct = 0\n",
    "            for x, y in zip(synonymy_score, labels[0]):\n",
    "                if x*0.8 <= y:\n",
    "                    syn_correct += 1\n",
    "                    syn_el_count += 1\n",
    "\n",
    "                else:\n",
    "                    syn_el_count += 1\n",
    "            \n",
    "            ant_el_count = 0\n",
    "            ant_correct = 0\n",
    "            for x, y in zip(antonymy_score, labels[1]):\n",
    "                if x*0.8 <= y:\n",
    "                    ant_correct += 1\n",
    "                    ant_el_count += 1\n",
    "\n",
    "                else:\n",
    "                    ant_el_count += 1\n",
    "            \n",
    "        syn_acc = (syn_correct/syn_el_count) * 100\n",
    "        syn_test_acc_list.append(syn_acc)\n",
    "        \n",
    "        ant_acc = (ant_correct/ant_el_count) * 100\n",
    "        ant_test_acc_list.append(ant_acc)\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        syn_test_epoch_loss = sum(syn_test_losses)/test_total\n",
    "        ant_test_epoch_loss = sum(ant_test_losses)/test_total\n",
    "        \n",
    "        syn_epoch_acc = sum(syn_test_acc_list)/test_total\n",
    "        ant_epoch_acc = sum(ant_test_acc_list)/test_total\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss is: {test_epoch_loss}\")\n",
    "        print(f\"Total Epoch Synonym Testing accuracy is: {syn_epoch_acc}\")\n",
    "        print(f\"Total Epoch Antonym Testing accuracy is: {ant_epoch_acc}\")\n",
    "\n",
    "    return test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, syn_epoch_acc, ant_epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload\n",
    "\n",
    "def fit(model, lr, epochs):\n",
    "\n",
    "    #define the optimizer\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "    #empty list to hold loss per epoch\n",
    "    train_epoch_losses = []\n",
    "    syn_train_epoch_losses = []\n",
    "    ant_train_epoch_losses = []\n",
    "\n",
    "    test_epoch_losses = []\n",
    "    syn_test_epoch_losses = []\n",
    "    ant_test_epoch_losses = []\n",
    "\n",
    "\n",
    "    syn_test_epoch_accuracies = []\n",
    "    ant_test_epoch_accuracies = []\n",
    "    test_epoch_accuracies = []\n",
    "\n",
    "    #pce_test_epoch_r2 = []\n",
    "    #voc_test_epoch_r2 = []\n",
    "    #jsc_test_epoch_r2 = []\n",
    "    #ff_test_epoch_r2 = []\n",
    "    #test_epoch_r2s = []\n",
    "\n",
    "    save_epochs = np.arange(0, num_epochs, 5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('On epoch ', epoch)\n",
    "    \n",
    "        #save_dir = \"/Users/Thomas/Desktop/BETO2020-Remote/Ant_Syn_Scraping/\"\n",
    "        #model_name = \"Phase_I_II_NN\"\n",
    "        #model_path = save_dir+model_name+'*.pt'\n",
    "        #if epoch < 10:\n",
    "            #save_path = save_dir + model_name + '_epoch0' + str(epoch) + '.pt'\n",
    "        #else:\n",
    "            #save_path = save_dir + model_name + '_epoch' + str(epoch) + '.pt'\n",
    "        \n",
    "#     if glob.glob(model_path) != []:\n",
    "#         model_states = glob.glob(model_path)\n",
    "#         model_states = sorted(model_states)\n",
    "#         previous_model = model_states[-1]\n",
    "        \n",
    "#         model, optimizer = nuts.load_trained_model(previous_model, model, optimizer)\n",
    "    \n",
    "        train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss = Phase_I_train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "        train_epoch_losses.append(train_epoch_loss)\n",
    "        syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "        ant_train_epoch_losses.append(ant_train_epoch_loss)\n",
    "\n",
    "        test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, syn_epoch_acc, ant_epoch_acc = Phase_I_eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "\n",
    "        test_epoch_losses.append(test_epoch_loss)\n",
    "        syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "        ant_test_epoch_losses.append(ant_test_epoch_loss)\n",
    "\n",
    "        #tot_tst_loss = sum(test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, label_test_epoch_loss)\n",
    "        #test_epoch_losses.append(tot_tst_loss)\n",
    "\n",
    "        syn_test_epoch_accuracies.append(syn_epoch_acc)\n",
    "        ant_test_epoch_accuracies.append(ant_epoch_acc)\n",
    "\n",
    "        tot_test_acc = (syn_epoch_acc + ant_epoch_acc)\n",
    "        test_epoch_accuracies.append(tot_test_acc)\n",
    "\n",
    "        print('Finished epoch ', epoch)\n",
    "\n",
    "    best_loss_indx = test_epoch_losses.index(min(test_epoch_losses))\n",
    "    best_acc_indx = test_epoch_accuracies.index(min(test_epoch_accuracies))\n",
    "\n",
    "    fit_results = {\n",
    "        'lr': lr,\n",
    "        'best_loss_epoch': best_loss_indx,\n",
    "        'best_acc_epoch': best_acc_indx,\n",
    "        #'best_r2_epoch': best_r2_indx,\n",
    "        'syn_loss': syn_test_epoch_losses,\n",
    "        'ant_loss': ant_test_epoch_losses,\n",
    "        'test_losses': test_epoch_losses,        \n",
    "        'syn_acc': syn_test_epoch_accuracies,\n",
    "        'ant_acc': ant_test_epoch_accuracies,\n",
    "        'test_accs': test_epoch_accuracies,\n",
    "        #'pce_r2': pce_test_epoch_r2,\n",
    "        #'voc_r2': voc_test_epoch_r2,\n",
    "        #'jsc_r2': jsc_test_epoch_r2,\n",
    "        #'ff_r2': ff_test_epoch_r2,\n",
    "        #'test_r2s': test_epoch_r2s,\n",
    "        'train_syn_loss': syn_train_epoch_losses,\n",
    "        'train_ant_loss': ant_train_epoch_losses,\n",
    "    }\n",
    "\n",
    "    return fit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    \"\"\"\n",
    "    Through the torch module.apply() function, this initialization is\n",
    "    recursively passed to all submodules and layers in the model\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(model) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(model.weight)\n",
    "        torch.nn.init.uniform_(model.bias)\n",
    "        \n",
    "    if type(model) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(model.weight)\n",
    "        torch.nn.init.uniform_(model.bias)\n",
    "        \n",
    "    if type(model) == nn.BatchNorm2d:\n",
    "        torch.nn.init.xavier_uniform_(model.weight)\n",
    "        torch.nn.init.uniform_(model.bias)\n",
    "        \n",
    "    if type(model) == nn.BatchNorm1d:\n",
    "        torch.nn.init.uniform_(model.weight) #can't use Xavier b/c of fan-in/out\n",
    "        torch.nn.init.uniform_(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phase_I_NN(\n",
       "  (embedded): Embedding_Pre_Trained_Weights(\n",
       "    (embedding): Embedding(2990, 50)\n",
       "  )\n",
       "  (hidden_layers): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "    (2): Linear(in_features=100, out_features=300, bias=True)\n",
       "    (3): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (S_branch): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (2): Softplus(beta=1, threshold=20)\n",
       "    (3): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (A_branch): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (2): Softplus(beta=1, threshold=20)\n",
       "    (3): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Phase_I_NN(in_dims = 50, out_dims = 2, common = common, model1 = model1).to(device)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing Loss is: 1.0004943067377263\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.001582140272314\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0239478566429832\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.000835288654674\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0003687089139766\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0015276941386135\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0011907003142617\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0005273114551196\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0010187354954807\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.0238485444675793\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0003850351680408\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.000657157464461\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.000370448285883\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0234954357147217\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0021752606738696\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0011040730909868\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.0015596422282131\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.001116850159385\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.0009120594371448\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0008401653983376\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.0012809146534314\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0004461461847478\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0010910630226135\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0006744048812173\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0002002174204045\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.000861650163477\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.0007765834981746\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0005883574485779\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0007882064039058\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.0229925459081477\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0009097186001865\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0003608031706377\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0029195655475964\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.0010131218216636\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0005314512686296\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0018538399176165\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0007408640601418\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0009472370147705\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0008924874392422\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0007072362032803\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.022882412780415\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0008969686248086\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.024077751419761\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Testing Loss is: 1.0007822676138445\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0007926496592434\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0007526820356196\n",
      "Total Epoch Synonym Testing accuracy is: 4.545454545454546\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0007033456455579\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0004998987371272\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0238403948870571\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 0.0\n",
      "Total Epoch Testing Loss is: 1.0013957077806646\n",
      "Total Epoch Synonym Testing accuracy is: 0.0\n",
      "Total Epoch Antonym Testing accuracy is: 4.545454545454546\n"
     ]
    }
   ],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "syn_train_epoch_losses = []\n",
    "ant_train_epoch_losses = []\n",
    "\n",
    "test_epoch_losses = []\n",
    "syn_test_epoch_losses = []\n",
    "ant_test_epoch_losses = []\n",
    "\n",
    "syn_test_epoch_accuracies = []\n",
    "ant_test_epoch_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, syn_train_epoch_loss, ant_train_epoch_loss  = Phase_I_train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    syn_train_epoch_losses.append(syn_train_epoch_loss)\n",
    "    ant_train_epoch_losses.append(ant_train_epoch_loss)\n",
    "   \n",
    "    test_epoch_loss, syn_test_epoch_loss, ant_test_epoch_loss, syn_epoch_acc, ant_epoch_acc = Phase_I_eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    syn_test_epoch_losses.append(syn_test_epoch_loss)\n",
    "    ant_test_epoch_losses.append(ant_test_epoch_loss)\n",
    "    \n",
    "    syn_test_epoch_accuracies.append(syn_epoch_acc)\n",
    "    ant_test_epoch_accuracies.append(ant_epoch_acc)\n",
    "    #test_epoch_losses.append(test_epoch_loss)\n",
    "    #syn_test_epoch_losses.append(syn_test_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
