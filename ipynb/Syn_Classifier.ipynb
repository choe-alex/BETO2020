{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switching directories for easy access to the data\n",
    "data = '/Users/alexchoe/Desktop/Capstone/m2py-master/data/all_abstracts_model/'\n",
    "os.chdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'study',\n",
       " 'the',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'neutron',\n",
       " 'star',\n",
       " 'matter',\n",
       " 'and',\n",
       " 'stars',\n",
       " 'in',\n",
       " 'relativistic',\n",
       " 'Brueckner-Hartree-Fock',\n",
       " 'theory.',\n",
       " 'Adopting',\n",
       " 'equation',\n",
       " 'state',\n",
       " '(EOS)',\n",
       " 'theory,',\n",
       " 'we',\n",
       " 'calculate',\n",
       " 'under',\n",
       " 'beta',\n",
       " 'equilibrium',\n",
       " 'condition',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'density.',\n",
       " 'perform',\n",
       " 'Thomas-Fermi',\n",
       " 'calculation',\n",
       " 'crust',\n",
       " 'region',\n",
       " 'taking',\n",
       " 'into',\n",
       " 'account',\n",
       " 'change',\n",
       " 'nuclear',\n",
       " 'shapes',\n",
       " 'around',\n",
       " 'density,',\n",
       " 'ρ{variant}',\n",
       " '∼',\n",
       " 'compare',\n",
       " 'results',\n",
       " 'with',\n",
       " 'dense',\n",
       " 'non-relativistic',\n",
       " 'many',\n",
       " 'body',\n",
       " 'The',\n",
       " 'qualitative',\n",
       " 'feature',\n",
       " 'is',\n",
       " 'consistent',\n",
       " 'previous',\n",
       " 'works.',\n",
       " 'EOS',\n",
       " 'found',\n",
       " 'to',\n",
       " 'be',\n",
       " 'stiffer',\n",
       " 'than',\n",
       " 'EOSs',\n",
       " 'that',\n",
       " 'proton',\n",
       " 'fraction',\n",
       " 'high',\n",
       " 'density',\n",
       " 'theory',\n",
       " 'quite',\n",
       " 'large',\n",
       " 'as',\n",
       " 'compared',\n",
       " 'those',\n",
       " 'calculations',\n",
       " 'allow',\n",
       " 'direct',\n",
       " 'process,',\n",
       " 'which',\n",
       " 'crucial',\n",
       " 'for',\n",
       " 'rapid',\n",
       " 'cooling',\n",
       " 'stars,',\n",
       " 'relatively',\n",
       " 'massive',\n",
       " 'stars.',\n",
       " '©',\n",
       " '1995',\n",
       " 'Differential',\n",
       " 'cross',\n",
       " 'sections',\n",
       " 'elastic',\n",
       " 'scattering',\n",
       " 'photons',\n",
       " 'by',\n",
       " '12C',\n",
       " '16O',\n",
       " 'are',\n",
       " 'measured',\n",
       " 'at',\n",
       " '58',\n",
       " '75',\n",
       " 'MeV',\n",
       " 'photon',\n",
       " 'energies',\n",
       " '45°',\n",
       " '150°',\n",
       " 'angles',\n",
       " 'using',\n",
       " 'tagged',\n",
       " 'beam',\n",
       " '95',\n",
       " 'ring',\n",
       " 'small-angle',\n",
       " 'data',\n",
       " 'used',\n",
       " 'test',\n",
       " 'predictions',\n",
       " 'obtained',\n",
       " 'from',\n",
       " 'total',\n",
       " 'photoabsorption,',\n",
       " 'whereas',\n",
       " 'large-angle',\n",
       " 'provide',\n",
       " 'information',\n",
       " 'on',\n",
       " 'M1',\n",
       " 'E1',\n",
       " 'mixing',\n",
       " 'ratio',\n",
       " 'electromagnetic',\n",
       " 'polarizabilities.',\n",
       " 'Our',\n",
       " 'experimental',\n",
       " 'result',\n",
       " 'Θ',\n",
       " '=',\n",
       " '90°,',\n",
       " 'polarizabilities',\n",
       " 'predominantly',\n",
       " 'electric',\n",
       " 'multipolarity',\n",
       " 'not',\n",
       " 'significantly',\n",
       " 'different',\n",
       " 'values,',\n",
       " 'has',\n",
       " 'been',\n",
       " 'confirmed',\n",
       " 'experimentally',\n",
       " 'higher',\n",
       " 'level',\n",
       " 'confidence.',\n",
       " 'present',\n",
       " 'algorithms',\n",
       " 'solve',\n",
       " 'hydrodynamics',\n",
       " '(3+1)-dimensional',\n",
       " 'situations',\n",
       " 'without',\n",
       " 'apparent',\n",
       " 'symmetry',\n",
       " 'simplify',\n",
       " 'solution.',\n",
       " 'In',\n",
       " 'simulations',\n",
       " 'heavy-ion',\n",
       " 'collisions,',\n",
       " 'these',\n",
       " 'numerical',\n",
       " 'schemes',\n",
       " 'have',\n",
       " 'deal',\n",
       " 'physical',\n",
       " 'vacuum',\n",
       " 'equations',\n",
       " 'first',\n",
       " 'order',\n",
       " 'phase',\n",
       " 'transition',\n",
       " 'between',\n",
       " 'hadron',\n",
       " 'quark-gluon',\n",
       " 'plasma,',\n",
       " 'i.e.',\n",
       " 'rather',\n",
       " 'special',\n",
       " 'conditions',\n",
       " 'usually',\n",
       " 'confronted',\n",
       " 'with.',\n",
       " 'Therefore,',\n",
       " 'prior',\n",
       " 'applying',\n",
       " 'them',\n",
       " 'directly',\n",
       " 'simulation',\n",
       " 'one',\n",
       " 'should',\n",
       " 'investigate',\n",
       " 'their',\n",
       " 'performance',\n",
       " 'well-controlled',\n",
       " 'situations.',\n",
       " 'consider',\n",
       " 'here',\n",
       " 'one-dimensional',\n",
       " 'expansion',\n",
       " 'vacuum,',\n",
       " 'an',\n",
       " 'analytically',\n",
       " 'solvable',\n",
       " 'problem',\n",
       " 'incorporates',\n",
       " 'both',\n",
       " 'aspect',\n",
       " 'well',\n",
       " 'state.',\n",
       " 'dependence',\n",
       " 'lifetime',\n",
       " 'mixed',\n",
       " 'initial',\n",
       " 'energy',\n",
       " 'discussed.',\n",
       " 'section',\n",
       " 'producing',\n",
       " 'slow',\n",
       " 'protons',\n",
       " 'charged',\n",
       " 'current',\n",
       " 'deep',\n",
       " 'inelastic',\n",
       " 'neutrons',\n",
       " 'calculated',\n",
       " 'function',\n",
       " 'Bjorken',\n",
       " 'x',\n",
       " 'momentum.',\n",
       " 'standard',\n",
       " 'hadronization',\n",
       " 'models',\n",
       " 'based',\n",
       " 'upon',\n",
       " 'colour',\n",
       " 'neutralization',\n",
       " 'mechanism',\n",
       " 'appear',\n",
       " 'underestimate',\n",
       " 'rate',\n",
       " 'production',\n",
       " 'hydrogen.',\n",
       " 'presence',\n",
       " 'virtual',\n",
       " 'mesons',\n",
       " 'nucleon',\n",
       " 'leads',\n",
       " 'additional',\n",
       " 'production,',\n",
       " 'referred',\n",
       " 'spectator',\n",
       " 'process.',\n",
       " 'It',\n",
       " 'low',\n",
       " 'momenta',\n",
       " 'mechanisms',\n",
       " 'compete,',\n",
       " 'dominates',\n",
       " 'very',\n",
       " 'small',\n",
       " 'momenta,',\n",
       " 'while',\n",
       " 'color',\n",
       " 'larger',\n",
       " '1-2',\n",
       " 'GeV/c.',\n",
       " 'CERN',\n",
       " 'bubble',\n",
       " 'chamber',\n",
       " 'data.',\n",
       " 'model',\n",
       " 'predicts',\n",
       " 'sharp',\n",
       " 'increase',\n",
       " 'due',\n",
       " 'sea',\n",
       " 'quarks',\n",
       " 'mesons.',\n",
       " 'attachment',\n",
       " 'probability',\n",
       " 'P',\n",
       " 'Λ',\n",
       " 'particle',\n",
       " 'fragments',\n",
       " 'resulting',\n",
       " 'fission',\n",
       " 'heavy',\n",
       " 'produced',\n",
       " 'antiproton',\n",
       " 'annihilation',\n",
       " 'uranium',\n",
       " 'target,',\n",
       " 'estimated',\n",
       " 'basis',\n",
       " 'simplified',\n",
       " 'statistical',\n",
       " 'model.',\n",
       " 'A',\n",
       " 'smooth',\n",
       " 'generated',\n",
       " 'harmonic',\n",
       " 'oscillator',\n",
       " 'spectrum',\n",
       " 'means',\n",
       " 'Gaussian',\n",
       " 'averaging',\n",
       " 'employed',\n",
       " 'spacing',\n",
       " 'h',\n",
       " '{combining',\n",
       " 'short',\n",
       " 'stroke',\n",
       " 'various',\n",
       " 'analytic',\n",
       " 'expressions',\n",
       " 'terms',\n",
       " 'mass',\n",
       " 'number',\n",
       " 'used.',\n",
       " 'values',\n",
       " 'PH,',\n",
       " 'fragment,',\n",
       " 'depend',\n",
       " 'expression',\n",
       " 'Comparison',\n",
       " 'PH',\n",
       " 'made',\n",
       " 'fully',\n",
       " 'relativistic,',\n",
       " 'gauge',\n",
       " 'invariant',\n",
       " 'unitary',\n",
       " 'pion',\n",
       " 'photoproduction',\n",
       " 'constructed',\n",
       " 'including',\n",
       " 'π,',\n",
       " 'ω,',\n",
       " 'N',\n",
       " 'Δ',\n",
       " 'degrees',\n",
       " 'freedom.',\n",
       " 'detail',\n",
       " 'observables',\n",
       " 'view',\n",
       " 'several',\n",
       " 'sets',\n",
       " 'high-quality',\n",
       " 'were',\n",
       " 'through',\n",
       " 'resonance',\n",
       " 'recent',\n",
       " 'years.',\n",
       " 'Starting',\n",
       " 'model,',\n",
       " 'mean',\n",
       " 'medium',\n",
       " 'derived.',\n",
       " 'satisfy',\n",
       " 'unitarity',\n",
       " 'applied',\n",
       " 'neutral',\n",
       " 'same',\n",
       " 'parameter',\n",
       " 'set.',\n",
       " 'Compilation',\n",
       " 'levels',\n",
       " '18',\n",
       " '19',\n",
       " 'nuclei,',\n",
       " 'emphasis',\n",
       " 'review',\n",
       " 'material',\n",
       " 'leading',\n",
       " 'about',\n",
       " 'structure',\n",
       " 'systems.',\n",
       " 'strong',\n",
       " 'coupling',\n",
       " 'effective',\n",
       " 'vertex',\n",
       " 'associated',\n",
       " 'triangle',\n",
       " 'diagram.',\n",
       " 'This',\n",
       " 'strange',\n",
       " 'quarkonium',\n",
       " 'nonet',\n",
       " 'close',\n",
       " 'known',\n",
       " 'Using',\n",
       " 'transport',\n",
       " 'includes',\n",
       " 'explicitly',\n",
       " 'kaon',\n",
       " 'degree',\n",
       " 'freedom,',\n",
       " 'flow,',\n",
       " 'average',\n",
       " 'transverse',\n",
       " 'momentum',\n",
       " 'rapidity',\n",
       " 'flow',\n",
       " 'Au+Au',\n",
       " 'collisions',\n",
       " '1',\n",
       " '1.45',\n",
       " 'also',\n",
       " 'flows',\n",
       " 'collisions.',\n",
       " 'For',\n",
       " 'our',\n",
       " 'non-linear',\n",
       " 'mean-field',\n",
       " 'potential',\n",
       " 'reasonable',\n",
       " 'agreement',\n",
       " 'preliminary',\n",
       " 'collaboration.',\n",
       " 'pions,',\n",
       " 'treated',\n",
       " 'potential,',\n",
       " 'observe',\n",
       " 'clearly',\n",
       " 'being',\n",
       " 'parallel',\n",
       " 'anti-parallel',\n",
       " 'when',\n",
       " 'two',\n",
       " 'ions',\n",
       " 'central',\n",
       " 'semi-central',\n",
       " 'peripheral',\n",
       " 'kaons,',\n",
       " 'use',\n",
       " 'four',\n",
       " 'assumptions',\n",
       " 'matter,',\n",
       " 'approximation',\n",
       " 'chiral',\n",
       " 'Lagrangian,',\n",
       " 'impulse',\n",
       " 'KN',\n",
       " 'length,',\n",
       " 'containing',\n",
       " 'only',\n",
       " 'repulsive',\n",
       " 'vector',\n",
       " 'interaction.',\n",
       " 'pattern',\n",
       " 'sensitive',\n",
       " 'thus',\n",
       " 'provides',\n",
       " 'useful',\n",
       " 'studying',\n",
       " 'medium.',\n",
       " 'Data',\n",
       " 'rest',\n",
       " 'hydrogen',\n",
       " 'gas',\n",
       " 'target',\n",
       " 'NTP',\n",
       " 'collected',\n",
       " 'spectrometer',\n",
       " 'exposed',\n",
       " 'extracted',\n",
       " 'LEAR',\n",
       " 'accelerator',\n",
       " 'CERN.',\n",
       " 'reactions',\n",
       " 'pp',\n",
       " '→',\n",
       " 'studied.',\n",
       " 'analysis',\n",
       " 'angular',\n",
       " 'distribution',\n",
       " 'kaons',\n",
       " 'emitted',\n",
       " 'φ',\n",
       " 'decay',\n",
       " 'shown',\n",
       " 'reaction',\n",
       " 'occurs',\n",
       " 'essentially',\n",
       " 'states',\n",
       " '(2.46',\n",
       " '±',\n",
       " '0.23',\n",
       " '0.07)',\n",
       " '×',\n",
       " '10-4,',\n",
       " '1P1',\n",
       " 'compatible',\n",
       " 'zero.',\n",
       " 'Because',\n",
       " 'acceptance',\n",
       " 'limitations,',\n",
       " 'contributions',\n",
       " 'could',\n",
       " 'resolved',\n",
       " 'lower',\n",
       " 'upper',\n",
       " 'limits',\n",
       " 'middle',\n",
       " 'value',\n",
       " '(0.87',\n",
       " '0.21)',\n",
       " '10-4.',\n",
       " 'Also,',\n",
       " 'phase-space',\n",
       " 'events',\n",
       " '0.35)',\n",
       " '10-4',\n",
       " 'measured.',\n",
       " 'branching',\n",
       " 'ratios',\n",
       " 'S-',\n",
       " '(defined',\n",
       " 'dependent',\n",
       " 'way.',\n",
       " 'evaluation',\n",
       " 'rate,',\n",
       " 'combination',\n",
       " 'liquid',\n",
       " 'confirms',\n",
       " 'violation',\n",
       " 'rule',\n",
       " 'observed',\n",
       " 'other',\n",
       " 'experiments.',\n",
       " 'No',\n",
       " 'production.',\n",
       " 'Transport',\n",
       " 'coefficients',\n",
       " 'symmetrical',\n",
       " 'low-temperature',\n",
       " 'regime',\n",
       " 'modifications',\n",
       " 'quasi-particle',\n",
       " 'amplitudes',\n",
       " 'environment.',\n",
       " 'two-body',\n",
       " 'in-medium',\n",
       " 'nucleon-nucleon',\n",
       " 'section,',\n",
       " 'critical',\n",
       " 'temperature',\n",
       " 'superfluid',\n",
       " 'partial',\n",
       " 'wave',\n",
       " 'channels',\n",
       " 'evaluated',\n",
       " 'separable',\n",
       " 'form',\n",
       " 'As',\n",
       " 'precursor',\n",
       " 'effect',\n",
       " 'onset',\n",
       " 'phase,',\n",
       " 'show',\n",
       " 'non-trivial',\n",
       " 'decreasing',\n",
       " 'temperature.',\n",
       " 'considerably',\n",
       " 'modified',\n",
       " 'either',\n",
       " 'free',\n",
       " 'or',\n",
       " 'isotropic',\n",
       " 'residual',\n",
       " 'σ',\n",
       " '40',\n",
       " 'pion-nucleon',\n",
       " 'constant',\n",
       " 'studied',\n",
       " 'QCD',\n",
       " 'sum',\n",
       " 'rules.',\n",
       " 'Both',\n",
       " 'Borel',\n",
       " 'rules',\n",
       " 'finite',\n",
       " 'examine',\n",
       " 'dimensional',\n",
       " 'operators',\n",
       " '(up',\n",
       " 'dimension',\n",
       " '7)',\n",
       " 'corrections',\n",
       " 'operator',\n",
       " 'product',\n",
       " 'expansion.',\n",
       " 'Agreement',\n",
       " 'reached',\n",
       " 'greater',\n",
       " 'one,',\n",
       " 'where',\n",
       " '(Sn)',\n",
       " 'continuum',\n",
       " 'threshold',\n",
       " 'rule.',\n",
       " 'meson-baryon',\n",
       " 'interaction',\n",
       " 'strangeness',\n",
       " 'S',\n",
       " '-1',\n",
       " 'sector',\n",
       " 'Lagrangian.',\n",
       " 'Potentials',\n",
       " 'derived',\n",
       " 'this',\n",
       " 'Lagrangian',\n",
       " 'coupled-channel',\n",
       " 'low-energy',\n",
       " 'observables.',\n",
       " 'potentials',\n",
       " 'such',\n",
       " 'Born',\n",
       " 's-wave',\n",
       " 'length',\n",
       " 'given',\n",
       " 'up',\n",
       " 'comparison',\n",
       " 'available',\n",
       " 'hadronic',\n",
       " 'coupled',\n",
       " 'system,',\n",
       " 'Λ(1405)',\n",
       " 'resonance,',\n",
       " 'scattering,',\n",
       " 'decay.',\n",
       " 'Good',\n",
       " 'fits',\n",
       " 'estimates',\n",
       " 'previously',\n",
       " 'unknown',\n",
       " 'parameters',\n",
       " 'obtained.',\n",
       " 'Strong',\n",
       " 'shifts',\n",
       " 'widths',\n",
       " 'Σ-',\n",
       " 'atoms',\n",
       " 'optical',\n",
       " 'within',\n",
       " 'field',\n",
       " 'approach.',\n",
       " 'real',\n",
       " 'part',\n",
       " 'interior.',\n",
       " 'sufficient',\n",
       " 'establish',\n",
       " 'size',\n",
       " 'isovector',\n",
       " 'coupling.',\n",
       " 'Implications',\n",
       " 'There',\n",
       " 'exists',\n",
       " 'evidence',\n",
       " 'dibaryon',\n",
       " 'd′',\n",
       " 'quantum',\n",
       " 'numbers',\n",
       " 'JP',\n",
       " '0-,',\n",
       " 'T',\n",
       " '0',\n",
       " '2065',\n",
       " 'origin',\n",
       " 'narrow',\n",
       " 'peak',\n",
       " 'double',\n",
       " 'charge',\n",
       " 'exchange',\n",
       " 'cross-sections',\n",
       " 'nuclei.',\n",
       " 'system',\n",
       " 'constituent',\n",
       " 'quark',\n",
       " 'linear',\n",
       " 'confinement,',\n",
       " 'one-gluon',\n",
       " 'interactions',\n",
       " 'classify',\n",
       " 'all',\n",
       " 'possible',\n",
       " 'six',\n",
       " '0,',\n",
       " '3',\n",
       " 'excitations,',\n",
       " 'reduction',\n",
       " 'chains.',\n",
       " 'Hamiltonian',\n",
       " 'diagonalized',\n",
       " 'unique',\n",
       " '10',\n",
       " 'most',\n",
       " 'important',\n",
       " 'shell.',\n",
       " 'find,',\n",
       " 'parameters,',\n",
       " 'lies',\n",
       " 'above',\n",
       " '+',\n",
       " 'threshold.',\n",
       " 'possibility',\n",
       " 'describe',\n",
       " 'supposed',\n",
       " 'context',\n",
       " 'reduce',\n",
       " 'confinement',\n",
       " 'strength',\n",
       " 'however',\n",
       " 'expense',\n",
       " 'describing',\n",
       " 'negative',\n",
       " 'parity',\n",
       " 'resonances',\n",
       " 'analyze',\n",
       " '2,',\n",
       " 'effects',\n",
       " 'short-range',\n",
       " 'correlations',\n",
       " 'realistic',\n",
       " 'meson-exchange',\n",
       " 'single-particle',\n",
       " 'matrix',\n",
       " 'nuclei',\n",
       " 'investigated',\n",
       " 'analyzing',\n",
       " 'one-body',\n",
       " 'natural',\n",
       " 'orbits.',\n",
       " 'Basic',\n",
       " 'features',\n",
       " 'orbits',\n",
       " 'spectral',\n",
       " 'distributions',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'approximate',\n",
       " 'orbits,',\n",
       " 'exhibit',\n",
       " 'largest',\n",
       " 'occupation',\n",
       " 'probabilities.',\n",
       " 'investigation',\n",
       " 'high-momentum',\n",
       " 'components',\n",
       " 'however,',\n",
       " 'take',\n",
       " 'probabilities,',\n",
       " 'originating',\n",
       " 'Green',\n",
       " 'energies.',\n",
       " 'An',\n",
       " 'nucleus',\n",
       " 'E',\n",
       " '155',\n",
       " 'MeV,',\n",
       " 'respectively,',\n",
       " 'performed.',\n",
       " 'Among',\n",
       " 'additions',\n",
       " 'existing',\n",
       " 'scheme,',\n",
       " 'new',\n",
       " 'band,',\n",
       " 'almost',\n",
       " 'identical',\n",
       " 'strongly',\n",
       " 'deformed',\n",
       " '(β2',\n",
       " '0.42)',\n",
       " '13',\n",
       " '2+]',\n",
       " 'band',\n",
       " 'recently',\n",
       " 'discovered',\n",
       " 'established.',\n",
       " 'theoretical',\n",
       " 'Lu',\n",
       " 'isotopes,',\n",
       " 'carried',\n",
       " 'out',\n",
       " 'detailed',\n",
       " 'surfaces',\n",
       " 'specific',\n",
       " 'configurations.',\n",
       " 'By',\n",
       " 'diabatic',\n",
       " 'treatment',\n",
       " 'crossings',\n",
       " 'configurations',\n",
       " 'identified',\n",
       " 'throughout',\n",
       " 'deformation',\n",
       " 'space',\n",
       " 'spin.',\n",
       " 'general',\n",
       " 'local',\n",
       " 'minima',\n",
       " 'considerable',\n",
       " 'nonaxial',\n",
       " 'coexist',\n",
       " 'normal',\n",
       " 'global',\n",
       " 'minimum.',\n",
       " 'depth',\n",
       " 'configuration.',\n",
       " 'analysed',\n",
       " 'discussed',\n",
       " 'orientation',\n",
       " 'relative',\n",
       " 'rotation',\n",
       " 'axis.',\n",
       " 'belong',\n",
       " 'group',\n",
       " 'superdeformed',\n",
       " 'triaxial',\n",
       " 'structures,',\n",
       " 'expected',\n",
       " 'certain',\n",
       " 'favourable',\n",
       " 'combinations',\n",
       " 'numbers.',\n",
       " 'intermediate',\n",
       " 'ion',\n",
       " 'hot',\n",
       " 'systems',\n",
       " 'formed',\n",
       " 'may',\n",
       " 'disassemble',\n",
       " 'any',\n",
       " 'three',\n",
       " 'distinct',\n",
       " 'mechanisms,',\n",
       " 'namely,',\n",
       " 'sequential',\n",
       " 'fission,',\n",
       " 'prompt',\n",
       " 'multifragmentation',\n",
       " 'evaporation.',\n",
       " 'considered',\n",
       " 'correlation',\n",
       " 'functions',\n",
       " '(IMFs)',\n",
       " 'spatial-temporal',\n",
       " 'extent',\n",
       " 'emission',\n",
       " 'process',\n",
       " 'exploited',\n",
       " 'delineate',\n",
       " 'break-up',\n",
       " 'scenarios.',\n",
       " 'Several',\n",
       " 'aspects',\n",
       " 'technique',\n",
       " 'calculating',\n",
       " 'interactions,',\n",
       " 'avoids',\n",
       " 'perturbation',\n",
       " 'eliminating',\n",
       " 'unlinked',\n",
       " 'graphs,',\n",
       " 'investigated.',\n",
       " 'course',\n",
       " 'p-shell',\n",
       " 'Reid',\n",
       " 'Soft',\n",
       " 'Paris,',\n",
       " 'coordinate',\n",
       " 'Bonn',\n",
       " 'interactions.',\n",
       " 'convergence',\n",
       " 'approaches',\n",
       " 'summing',\n",
       " 'folded',\n",
       " 'diagram',\n",
       " 'series,',\n",
       " 'Suzuki',\n",
       " 'Lee',\n",
       " 'compared.',\n",
       " 'latter,',\n",
       " 'cases',\n",
       " 'studied,',\n",
       " 'superior.',\n",
       " 'find',\n",
       " 'removing',\n",
       " 'influence',\n",
       " 'graphs',\n",
       " 'excitation',\n",
       " 'characterized',\n",
       " 'shift',\n",
       " 'binding',\n",
       " 'energy.',\n",
       " 'addition',\n",
       " 'note',\n",
       " 'agree',\n",
       " 'experiment.',\n",
       " 'Potential',\n",
       " 'conjunction',\n",
       " 'multi-modal',\n",
       " 'random',\n",
       " 'Müller',\n",
       " 'indicate',\n",
       " 'existence',\n",
       " 'modes',\n",
       " 'compound',\n",
       " 'time',\n",
       " 'splitting',\n",
       " 'mode',\n",
       " 'demonstrated',\n",
       " 'theoretically.',\n",
       " 'third',\n",
       " 'already',\n",
       " 'proposed',\n",
       " 'heavier',\n",
       " 'nonnegligible',\n",
       " 'yield',\n",
       " 'more',\n",
       " 'asymmetric',\n",
       " 'observed.',\n",
       " 'fragment',\n",
       " 'results.',\n",
       " 'Coincidence',\n",
       " 'measurements',\n",
       " '4He',\n",
       " '1H',\n",
       " 'second',\n",
       " '(',\n",
       " '1H)',\n",
       " '905',\n",
       " '1030',\n",
       " '121Sb',\n",
       " '27Al',\n",
       " '550',\n",
       " '750',\n",
       " 'All',\n",
       " 'lead',\n",
       " 'composite',\n",
       " 'facilitates',\n",
       " 'comparisons',\n",
       " 'matched',\n",
       " 'reactions.',\n",
       " '63Cu',\n",
       " 'reaction,',\n",
       " 'particle-particle',\n",
       " 'coincidences',\n",
       " 'dominated',\n",
       " 'processes',\n",
       " 'evaporation',\n",
       " 'residues.',\n",
       " 'half',\n",
       " 'residue',\n",
       " 'These',\n",
       " 'arise',\n",
       " 'subsets',\n",
       " 'sections.',\n",
       " 'individual',\n",
       " 'chain',\n",
       " 'lengths',\n",
       " '2',\n",
       " 'each',\n",
       " 'multiplicities',\n",
       " 'subset',\n",
       " 'residues',\n",
       " 'similar',\n",
       " 'reactions,',\n",
       " 'contrast',\n",
       " 'spin-dependent',\n",
       " 'entrance',\n",
       " 'channel',\n",
       " 'Coherent',\n",
       " 'deuteron',\n",
       " 'adopting',\n",
       " 'nonrelativistic',\n",
       " 'Results',\n",
       " 'polarization',\n",
       " 'corresponding',\n",
       " 'polarized',\n",
       " 'and/or',\n",
       " 'presented',\n",
       " 'nonresonant',\n",
       " 'sensitivity',\n",
       " 'multipoles',\n",
       " 'elementary',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Opening contents of Word2Vec model\n",
    "model = Word2Vec.load(\"all_abstract_model.model\")\n",
    "vocabulary = list(model.wv.vocab)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.51539   ,   4.103919  ,  -1.8394607 ,  -6.114477  ,\n",
       "         6.1308594 ,   6.0427675 ,  -1.1978918 ,   1.855273  ,\n",
       "        -4.160244  ,  -3.742103  ,  -0.5794198 ,   9.883354  ,\n",
       "        -4.3117127 ,   4.3782854 ,   3.5364165 ,  -4.9079742 ,\n",
       "        -2.0147028 ,   5.517852  ,  -2.5264072 ,   0.60432535,\n",
       "        -0.90231055,   2.0508857 ,  -1.0533296 ,  -4.2132974 ,\n",
       "         4.2740326 ,  -1.8519586 ,   2.927174  ,  -1.760242  ,\n",
       "       -13.595478  ,   6.390037  ,   5.449058  ,   1.0620196 ,\n",
       "         1.1603653 ,  -1.8674527 ,  -7.909827  ,   3.757318  ,\n",
       "        -0.7085782 ,  -1.8104229 ,  -0.7972807 ,   0.24420857,\n",
       "        -0.02632353,  -5.917606  ,  10.481008  ,  -3.1318085 ,\n",
       "        -1.7313521 ,   2.7741985 ,   3.4522643 ,  -4.4935923 ,\n",
       "         1.5952134 ,   0.79401314], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.__getitem__('study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/alexchoe/Desktop/Capstone/BETO2020-master/data/carbon/'\n",
    "os.chdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking in data as a dataframe for easy pre-processing\n",
    "df = pd.read_excel('Carbon_SynAntList_Full_Refined.xlsx', skiprows = 1)\n",
    "carbon_df = df.rename(columns = {'Unnamed: 0':'index', 0:'word 1', 1:'word 2', 2:'relationship', 'Unnamed: 4':'label'})\n",
    "carbon_df = carbon_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask to only keep strong word pair relationships\n",
    "condition = carbon_df['label'] != 0\n",
    "keep = (condition)\n",
    "carbon_df = carbon_df[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carbon'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carbon_df['word 1'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1b1c4fe5fc5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Restructuring the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarbon_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcarbon_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word 1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarbon_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word 1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcarbon_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word 2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarbon_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word 2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "#Restructuring the dataframe\n",
    "for i in range(len(carbon_df)):\n",
    "    carbon_df['word 1'].iloc[i] = model.wv.__getitem__(carbon_df['word 1'].iloc[i])\n",
    "    carbon_df['word 2'].iloc[i] = model.wv.__getitem__(carbon_df['word 2'].iloc[i])\n",
    "    \n",
    "     #except KeyError:\n",
    "        #carbon_df['word 1'].iloc[i] = '0'\n",
    "        #carbon_df['word 2'].iloc[i] = '0'\n",
    "    \n",
    "    if carbon_df['label'].iloc[i] == str(syn):\n",
    "        carbon_df['label'].iloc[i] = 1\n",
    "        \n",
    "    else: \n",
    "        carbon_df['label'].iloc[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "learning_rate = 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = W2V_df[['Word 1', 'Word 2']] #Input features used to make predictions\n",
    "Y = W2V_df[['Syn','NonSyn']] #Target features to be predicted \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, shuffle = True) #split dataset into separate testing and training datasets\n",
    "\n",
    "syn_train = y_train['Syn']\n",
    "syn_test = y_test['Syn']\n",
    "nonsyn_train = y_train['NonSyn']\n",
    "nonsyn_test = y_test['NonSyn']\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train.values.astype(np.float32)) #convert pd.DataFrame -> np.ndarray -> torch.tensor\n",
    "syn_train_tensor = torch.tensor(syn_train.values.astype(np.float32))\n",
    "ant_train_tensor = torch.tensor(syn_train.values.astype(np.float32))\n",
    "\n",
    "#create tensor with features and targets\n",
    "train_tensor = torch.utils.data.TensorDataset(x_train_tensor, syn_train_tensor, nonsyn_train_tensor)\n",
    "#create iterable dataset with batches\n",
    "training_data_set = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test.values.astype(np.float32))\n",
    "syn_test_tensor = torch.tensor(syn_test.values.astype(np.float32))\n",
    "nonsyn_test_tensor = torch.tensor(nonsyn_test.values.astype(np.float32))\n",
    "\n",
    "test_tensor = torch.utils.data.TensorDataset(x_test_tensor, syn_test_tensor)\n",
    "testing_data_set = torch.utils.data.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the neural network\n",
    "class syn_NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dims, out_dims):\n",
    "        \n",
    "        #embedding layer\n",
    "        self.em_layer = nn.Linear(in_dims, out_dims)\n",
    "\n",
    "        #hidden layers\n",
    "        self.h_layer1 = nn.Linear(out_dims, 32)\n",
    "        self.h_layer2 = nn.Linear(32, 16)\n",
    "        \n",
    "        #output layer\n",
    "        self.o_layer = nn.Linear(16, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #data enters embedding layer\n",
    "        out = self.em_layer(x)\n",
    "        \n",
    "        #embedded data is passed to hidden layers\n",
    "        out = self.h_layer1(out)\n",
    "        out = self.h_layer2(out)\n",
    "        \n",
    "        #embedded data is passed to output layers\n",
    "        syn_out = self.o_layer(out)\n",
    "        \n",
    "        return syn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_data_set, optimizer):\n",
    "    train_epoch_loss = []\n",
    "    syn_train_epoch_loss = []\n",
    "    \n",
    "    syn_losses = []\n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    syn_criterion = PhysLoss.ThresholdedMSELoss(lower = 0, upper = 6)\n",
    "    \n",
    "    for train_data in training_data_set:\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        syn_out = model(train_data) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        syn_loss = syn_criterion(predictions = syn_out)\n",
    "        \n",
    "        total_loss = syn_loss\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([syn_loss])\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        syn_train_losses.append(pce_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    syn_train_epoch_loss.append(sum(ff_train_losses)/train_total)\n",
    "    \n",
    "    #update progress bar\n",
    "    print(f\"Total Epoch Training Loss = {train_epoch_loss}\")\n",
    "    \n",
    "    return train_epoch_loss, syn_train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    syn_criterion = PhysLoss.ThresholdedMSELoss(lower = 0, upper = 6)\n",
    "    accuracy = PhysLoss.MAPE()\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        syn_test_losses = []\n",
    "        syn_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        for inputs in testing_data_set:\n",
    "            inputs = inputs.to(device)\n",
    "            syn_labels = syn_labels.to(device)\n",
    "\n",
    "            syn_out = model(inputs)\n",
    "\n",
    "            # calculate loss per batch of testing data\n",
    "            syn_test_loss = syn_criterion(syn_out)\n",
    "            \n",
    "            test_loss = pce_test_loss + voc_test_loss + jsc_test_loss + ff_test_loss\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            syn_test_losses.append(syn_test_loss.item())\n",
    "            test_total += 1 \n",
    "\n",
    "            syn_acc = accuracy(syn_out)\n",
    "            syn_test_acc_list.append(syn_acc.item())\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        syn_test_epoch_loss = sum(syn_test_losses)/test_total\n",
    "        \n",
    "        syn_epoch_acc = sum(syn_test_acc_list)/test_total\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss = {test_epoch_loss}\")\n",
    "        print(f\"Epoch MAPE: Syn = {syn_epoch_acc}\")\n",
    "    return test_epoch_loss, syn_test_epoch_loss, syn_epoch_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
