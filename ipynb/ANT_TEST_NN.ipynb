{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switching directories for easy access to the data\n",
    "data = '/Users/alexchoe/Desktop/Capstone/m2py-master/data/all_abstracts_model/'\n",
    "os.chdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening contents of Word2Vec model\n",
    "model = Word2Vec.load(\"all_abstract_model.model\")\n",
    "vocabulary = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/alexchoe/Desktop/Capstone/BETO2020-master/data/carbon/'\n",
    "os.chdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking in data as a dataframe for easy pre-processing\n",
    "df = pd.read_excel('Carbon_SynAntList_Full_Refined.xlsx', skiprows = 1, nrows=2000)\n",
    "carbon_df = df.rename(columns = {'Unnamed: 0':'index', 0:'word 1', 1:'word 2', 2:'relationship', 'Unnamed: 4':'label'})\n",
    "carbon_df = carbon_df.fillna(0)\n",
    "carbon_df = carbon_df[1:]\n",
    "\n",
    "#finding which words are in the pd but not in vocabulary1\n",
    "list1 = list(carbon_df['word 1'])\n",
    "list2 = list(carbon_df['word 2'])\n",
    "missing = list((set(list1).difference(vocabulary))) + list((set(list2).difference(vocabulary)))\n",
    "\n",
    "#keeping only the rows in the pd that have words in vocabulary1\n",
    "carbon_df = carbon_df[~carbon_df['word 1'].isin(missing)]\n",
    "carbon_df = carbon_df[~carbon_df['word 2'].isin(missing)]\n",
    "\n",
    "#reseting indices after mask\n",
    "carbon_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexchoe/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(carbon_df)):\n",
    "    carbon_df['word 1'].iloc[i] = model.wv.__getitem__(str(carbon_df['word 1'].iloc[i]))\n",
    "    carbon_df['word 2'].iloc[i] = model.wv.__getitem__(str(carbon_df['word 2'].iloc[i]))\n",
    "    \n",
    "    if carbon_df['relationship'].iloc[i] == 'ant' and carbon_df['label'].iloc[i] == 1:\n",
    "        carbon_df['relationship'].iloc[i] = 1\n",
    "    else:\n",
    "        carbon_df['relationship'].iloc[i] = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>word 1</th>\n",
       "      <th>word 2</th>\n",
       "      <th>relationship</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.0246488, -5.6508703, -1.4263288, -3.1607409...</td>\n",
       "      <td>[0.3912109, -2.6639938, -0.4191871, -0.3595066...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[1.0246488, -5.6508703, -1.4263288, -3.1607409...</td>\n",
       "      <td>[0.67807263, -0.0778522, 3.3564792, -1.8280518...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[1.0246488, -5.6508703, -1.4263288, -3.1607409...</td>\n",
       "      <td>[-0.40175724, 0.66337395, -1.5072205, -1.73012...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[1.0246488, -5.6508703, -1.4263288, -3.1607409...</td>\n",
       "      <td>[2.6374276, -0.8799803, 1.9580756, -3.1686919,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[1.0246488, -5.6508703, -1.4263288, -3.1607409...</td>\n",
       "      <td>[-1.5558529, 2.824446, -3.416154, -0.963536, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1596</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>[-0.2082266, 1.1971192, 1.1562068, -5.095038, ...</td>\n",
       "      <td>[0.38347286, 0.7061271, -0.007815216, -1.64924...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1597</td>\n",
       "      <td>1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>[-0.2082266, 1.1971192, 1.1562068, -5.095038, ...</td>\n",
       "      <td>[-2.8429897, 1.9741825, -0.3001447, -5.303599,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1598</td>\n",
       "      <td>1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>[-0.2082266, 1.1971192, 1.1562068, -5.095038, ...</td>\n",
       "      <td>[0.26439202, 0.44663662, -0.2789563, -3.575421...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>[-0.2082266, 1.1971192, 1.1562068, -5.095038, ...</td>\n",
       "      <td>[-7.4225054, -2.0302646, 0.6465284, -7.604581,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1999</td>\n",
       "      <td>1999</td>\n",
       "      <td>[-0.2082266, 1.1971192, 1.1562068, -5.095038, ...</td>\n",
       "      <td>[-7.340902, -2.531378, 1.975088, -2.8851795, -...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1601 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      level_0  index                                             word 1  \\\n",
       "0           1      1  [1.0246488, -5.6508703, -1.4263288, -3.1607409...   \n",
       "1           2      2  [1.0246488, -5.6508703, -1.4263288, -3.1607409...   \n",
       "2           3      3  [1.0246488, -5.6508703, -1.4263288, -3.1607409...   \n",
       "3           4      4  [1.0246488, -5.6508703, -1.4263288, -3.1607409...   \n",
       "4           7      7  [1.0246488, -5.6508703, -1.4263288, -3.1607409...   \n",
       "...       ...    ...                                                ...   \n",
       "1596     1995   1995  [-0.2082266, 1.1971192, 1.1562068, -5.095038, ...   \n",
       "1597     1996   1996  [-0.2082266, 1.1971192, 1.1562068, -5.095038, ...   \n",
       "1598     1997   1997  [-0.2082266, 1.1971192, 1.1562068, -5.095038, ...   \n",
       "1599     1998   1998  [-0.2082266, 1.1971192, 1.1562068, -5.095038, ...   \n",
       "1600     1999   1999  [-0.2082266, 1.1971192, 1.1562068, -5.095038, ...   \n",
       "\n",
       "                                                 word 2  relationship label  \n",
       "0     [0.3912109, -2.6639938, -0.4191871, -0.3595066...             0     1  \n",
       "1     [0.67807263, -0.0778522, 3.3564792, -1.8280518...             0     1  \n",
       "2     [-0.40175724, 0.66337395, -1.5072205, -1.73012...             0     0  \n",
       "3     [2.6374276, -0.8799803, 1.9580756, -3.1686919,...             0     0  \n",
       "4     [-1.5558529, 2.824446, -3.416154, -0.963536, 0...             0     0  \n",
       "...                                                 ...           ...   ...  \n",
       "1596  [0.38347286, 0.7061271, -0.007815216, -1.64924...             0     0  \n",
       "1597  [-2.8429897, 1.9741825, -0.3001447, -5.303599,...             0     0  \n",
       "1598  [0.26439202, 0.44663662, -0.2789563, -3.575421...             0     0  \n",
       "1599  [-7.4225054, -2.0302646, 0.6465284, -7.604581,...             0     0  \n",
       "1600  [-7.340902, -2.531378, 1.975088, -2.8851795, -...             0     0  \n",
       "\n",
       "[1601 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carbon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_df.to_json('ANT_NN_DATA.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_df = pd.read_json('ANT_NN_DATA.json', dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "learning_rate = 0.008\n",
    "\n",
    "# Device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = carbon_df[['word 1', 'word 2']] #Input features used to make predictions\n",
    "Y = carbon_df[['relationship']] #Target feature to be predicted \n",
    " \n",
    "x_train, x_test, ant_train, ant_test = train_test_split(X,Y, test_size = 0.2, shuffle = True) #split dataset into separate testing and training datasets\n",
    "\n",
    "x_train.reset_index(inplace = True)\n",
    "x_test.reset_index(inplace = True)\n",
    "ant_train.reset_index(inplace = True)\n",
    "ant_test.reset_index(inplace = True)\n",
    "\n",
    "x1_train = x_train['word 1']\n",
    "x2_train = x_train['word 2']\n",
    "x1_test = x_test['word 1']\n",
    "x2_test = x_test['word 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_array = np.empty((len(x_train),100))\n",
    "x_train_df = pd.DataFrame(data=x_train_array[0:,:])\n",
    "\n",
    "x_test_array = np.empty((len(x_test),100))\n",
    "x_test_df = pd.DataFrame(data=x_test_array[0:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train_df)):\n",
    "    for u in range(len(x_train['word 2'][i])):\n",
    "        x_train_df.iloc[i,u] = x_train['word 1'][i][u]\n",
    "        \n",
    "for i in range(len(x_train_df)):\n",
    "    for u in range(len(x_train['word 2'][i])):\n",
    "        x_train_df.iloc[i,(50+u)] = x_train['word 2'][i][u]\n",
    "\n",
    "                \n",
    "for i in range(len(x_test_df)):\n",
    "    for u in range(len(x_test['word 1'][i])):\n",
    "        x_test_df.iloc[i,u] = x_test['word 1'][i][u]\n",
    "        \n",
    "                \n",
    "for i in range(len(x_test_df)):\n",
    "    for u in range(len(x_test['word 2'][i])):\n",
    "        x_test_df.iloc[i,(50+u)] = x_test['word 2'][i][u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-4.572905</td>\n",
       "      <td>-6.443137</td>\n",
       "      <td>5.794405</td>\n",
       "      <td>-6.471600</td>\n",
       "      <td>6.504431</td>\n",
       "      <td>2.951210</td>\n",
       "      <td>-3.090467</td>\n",
       "      <td>-1.286095</td>\n",
       "      <td>5.204089</td>\n",
       "      <td>2.941126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.953811</td>\n",
       "      <td>-0.791730</td>\n",
       "      <td>-0.853084</td>\n",
       "      <td>0.103855</td>\n",
       "      <td>0.111677</td>\n",
       "      <td>0.340079</td>\n",
       "      <td>0.946939</td>\n",
       "      <td>0.333773</td>\n",
       "      <td>-0.220313</td>\n",
       "      <td>-0.171673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1.777941</td>\n",
       "      <td>4.843503</td>\n",
       "      <td>0.542043</td>\n",
       "      <td>-3.549473</td>\n",
       "      <td>1.083318</td>\n",
       "      <td>2.146862</td>\n",
       "      <td>0.475878</td>\n",
       "      <td>-0.150475</td>\n",
       "      <td>2.156058</td>\n",
       "      <td>3.089100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424291</td>\n",
       "      <td>-0.353807</td>\n",
       "      <td>-0.194475</td>\n",
       "      <td>-0.373754</td>\n",
       "      <td>-0.674903</td>\n",
       "      <td>0.122289</td>\n",
       "      <td>1.001551</td>\n",
       "      <td>-0.616002</td>\n",
       "      <td>0.051963</td>\n",
       "      <td>-0.213525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.025998</td>\n",
       "      <td>0.289955</td>\n",
       "      <td>3.751968</td>\n",
       "      <td>3.400925</td>\n",
       "      <td>-3.493372</td>\n",
       "      <td>-1.015835</td>\n",
       "      <td>3.067935</td>\n",
       "      <td>-6.638695</td>\n",
       "      <td>2.008589</td>\n",
       "      <td>-2.815345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641664</td>\n",
       "      <td>-3.914162</td>\n",
       "      <td>4.155166</td>\n",
       "      <td>-1.623855</td>\n",
       "      <td>-0.231471</td>\n",
       "      <td>1.391196</td>\n",
       "      <td>3.749104</td>\n",
       "      <td>1.885215</td>\n",
       "      <td>0.500033</td>\n",
       "      <td>-0.718096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-2.260679</td>\n",
       "      <td>-0.688991</td>\n",
       "      <td>-2.192621</td>\n",
       "      <td>4.956737</td>\n",
       "      <td>1.757224</td>\n",
       "      <td>2.145969</td>\n",
       "      <td>2.782162</td>\n",
       "      <td>1.879095</td>\n",
       "      <td>-1.117045</td>\n",
       "      <td>-3.963360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241486</td>\n",
       "      <td>-0.036381</td>\n",
       "      <td>-0.050983</td>\n",
       "      <td>-0.038339</td>\n",
       "      <td>0.057630</td>\n",
       "      <td>0.076125</td>\n",
       "      <td>0.067552</td>\n",
       "      <td>-0.116768</td>\n",
       "      <td>0.017387</td>\n",
       "      <td>0.067028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-4.572905</td>\n",
       "      <td>-6.443137</td>\n",
       "      <td>5.794405</td>\n",
       "      <td>-6.471600</td>\n",
       "      <td>6.504431</td>\n",
       "      <td>2.951210</td>\n",
       "      <td>-3.090467</td>\n",
       "      <td>-1.286095</td>\n",
       "      <td>5.204089</td>\n",
       "      <td>2.941126</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.434420</td>\n",
       "      <td>1.636823</td>\n",
       "      <td>2.959791</td>\n",
       "      <td>3.318134</td>\n",
       "      <td>-0.927886</td>\n",
       "      <td>-0.782501</td>\n",
       "      <td>3.576220</td>\n",
       "      <td>1.751391</td>\n",
       "      <td>1.308552</td>\n",
       "      <td>4.317373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>-1.096448</td>\n",
       "      <td>1.880548</td>\n",
       "      <td>7.234420</td>\n",
       "      <td>-0.867733</td>\n",
       "      <td>-2.772371</td>\n",
       "      <td>-0.380264</td>\n",
       "      <td>-2.024324</td>\n",
       "      <td>-3.206657</td>\n",
       "      <td>1.550386</td>\n",
       "      <td>-0.245448</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.077198</td>\n",
       "      <td>-3.256073</td>\n",
       "      <td>2.812804</td>\n",
       "      <td>-4.099342</td>\n",
       "      <td>-7.988791</td>\n",
       "      <td>1.045651</td>\n",
       "      <td>7.723909</td>\n",
       "      <td>-0.491465</td>\n",
       "      <td>5.692377</td>\n",
       "      <td>6.619974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1276</td>\n",
       "      <td>3.952686</td>\n",
       "      <td>1.531852</td>\n",
       "      <td>-10.797078</td>\n",
       "      <td>5.054812</td>\n",
       "      <td>5.867758</td>\n",
       "      <td>1.440295</td>\n",
       "      <td>5.110829</td>\n",
       "      <td>-5.852190</td>\n",
       "      <td>-1.000157</td>\n",
       "      <td>-6.799130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122613</td>\n",
       "      <td>0.318446</td>\n",
       "      <td>0.914337</td>\n",
       "      <td>-0.571856</td>\n",
       "      <td>-1.264935</td>\n",
       "      <td>0.416868</td>\n",
       "      <td>2.980009</td>\n",
       "      <td>-0.325881</td>\n",
       "      <td>-0.850117</td>\n",
       "      <td>0.912961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1277</td>\n",
       "      <td>-0.382762</td>\n",
       "      <td>2.394328</td>\n",
       "      <td>1.349133</td>\n",
       "      <td>-0.072012</td>\n",
       "      <td>-2.511072</td>\n",
       "      <td>1.489475</td>\n",
       "      <td>2.405386</td>\n",
       "      <td>-5.643640</td>\n",
       "      <td>0.467026</td>\n",
       "      <td>-2.869704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304719</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.401125</td>\n",
       "      <td>-2.283455</td>\n",
       "      <td>-0.196228</td>\n",
       "      <td>-0.448581</td>\n",
       "      <td>1.024016</td>\n",
       "      <td>-0.235990</td>\n",
       "      <td>1.140137</td>\n",
       "      <td>1.433193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1278</td>\n",
       "      <td>-4.572905</td>\n",
       "      <td>-6.443137</td>\n",
       "      <td>5.794405</td>\n",
       "      <td>-6.471600</td>\n",
       "      <td>6.504431</td>\n",
       "      <td>2.951210</td>\n",
       "      <td>-3.090467</td>\n",
       "      <td>-1.286095</td>\n",
       "      <td>5.204089</td>\n",
       "      <td>2.941126</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.353548</td>\n",
       "      <td>2.297662</td>\n",
       "      <td>1.898378</td>\n",
       "      <td>1.595865</td>\n",
       "      <td>-3.843940</td>\n",
       "      <td>4.743205</td>\n",
       "      <td>8.232018</td>\n",
       "      <td>3.079020</td>\n",
       "      <td>-3.681024</td>\n",
       "      <td>4.366352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1279</td>\n",
       "      <td>0.597913</td>\n",
       "      <td>-3.294207</td>\n",
       "      <td>0.697818</td>\n",
       "      <td>6.261357</td>\n",
       "      <td>7.551962</td>\n",
       "      <td>3.178151</td>\n",
       "      <td>4.695152</td>\n",
       "      <td>-1.801616</td>\n",
       "      <td>2.589233</td>\n",
       "      <td>3.807180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.691856</td>\n",
       "      <td>1.368468</td>\n",
       "      <td>1.073762</td>\n",
       "      <td>-1.941056</td>\n",
       "      <td>0.127375</td>\n",
       "      <td>-1.134181</td>\n",
       "      <td>3.148027</td>\n",
       "      <td>2.247618</td>\n",
       "      <td>-2.232352</td>\n",
       "      <td>-2.663854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1          2         3         4         5         6   \\\n",
       "0    -4.572905 -6.443137   5.794405 -6.471600  6.504431  2.951210 -3.090467   \n",
       "1    -1.777941  4.843503   0.542043 -3.549473  1.083318  2.146862  0.475878   \n",
       "2     3.025998  0.289955   3.751968  3.400925 -3.493372 -1.015835  3.067935   \n",
       "3    -2.260679 -0.688991  -2.192621  4.956737  1.757224  2.145969  2.782162   \n",
       "4    -4.572905 -6.443137   5.794405 -6.471600  6.504431  2.951210 -3.090467   \n",
       "...        ...       ...        ...       ...       ...       ...       ...   \n",
       "1275 -1.096448  1.880548   7.234420 -0.867733 -2.772371 -0.380264 -2.024324   \n",
       "1276  3.952686  1.531852 -10.797078  5.054812  5.867758  1.440295  5.110829   \n",
       "1277 -0.382762  2.394328   1.349133 -0.072012 -2.511072  1.489475  2.405386   \n",
       "1278 -4.572905 -6.443137   5.794405 -6.471600  6.504431  2.951210 -3.090467   \n",
       "1279  0.597913 -3.294207   0.697818  6.261357  7.551962  3.178151  4.695152   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0    -1.286095  5.204089  2.941126  ... -0.953811 -0.791730 -0.853084   \n",
       "1    -0.150475  2.156058  3.089100  ... -0.424291 -0.353807 -0.194475   \n",
       "2    -6.638695  2.008589 -2.815345  ... -0.641664 -3.914162  4.155166   \n",
       "3     1.879095 -1.117045 -3.963360  ...  0.241486 -0.036381 -0.050983   \n",
       "4    -1.286095  5.204089  2.941126  ... -7.434420  1.636823  2.959791   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1275 -3.206657  1.550386 -0.245448  ... -2.077198 -3.256073  2.812804   \n",
       "1276 -5.852190 -1.000157 -6.799130  ...  0.122613  0.318446  0.914337   \n",
       "1277 -5.643640  0.467026 -2.869704  ... -0.304719  0.014149  0.401125   \n",
       "1278 -1.286095  5.204089  2.941126  ... -3.353548  2.297662  1.898378   \n",
       "1279 -1.801616  2.589233  3.807180  ... -0.691856  1.368468  1.073762   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0     0.103855  0.111677  0.340079  0.946939  0.333773 -0.220313 -0.171673  \n",
       "1    -0.373754 -0.674903  0.122289  1.001551 -0.616002  0.051963 -0.213525  \n",
       "2    -1.623855 -0.231471  1.391196  3.749104  1.885215  0.500033 -0.718096  \n",
       "3    -0.038339  0.057630  0.076125  0.067552 -0.116768  0.017387  0.067028  \n",
       "4     3.318134 -0.927886 -0.782501  3.576220  1.751391  1.308552  4.317373  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1275 -4.099342 -7.988791  1.045651  7.723909 -0.491465  5.692377  6.619974  \n",
       "1276 -0.571856 -1.264935  0.416868  2.980009 -0.325881 -0.850117  0.912961  \n",
       "1277 -2.283455 -0.196228 -0.448581  1.024016 -0.235990  1.140137  1.433193  \n",
       "1278  1.595865 -3.843940  4.743205  8.232018  3.079020 -3.681024  4.366352  \n",
       "1279 -1.941056  0.127375 -1.134181  3.148027  2.247618 -2.232352 -2.663854  \n",
       "\n",
       "[1280 rows x 100 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train_df.values.astype(np.float32)) #convert pd.DataFrame -> np.ndarray -> torch.tensor\n",
    "ant_train_tensor = torch.tensor(ant_train.values.astype(np.float32))\n",
    "#nonant_train_tensor = torch.tensor(nonant_train)\n",
    "\n",
    "#create tensor with features and targets\n",
    "train_tensor = torch.utils.data.TensorDataset(x_train_tensor, ant_train_tensor)\n",
    "#create iterable dataset with batches\n",
    "training_data_set = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_df.values.astype(np.float32))\n",
    "ant_test_tensor = torch.tensor(ant_test.values.astype(np.float32))\n",
    "#nonsyn_test_tensor = torch.tensor(nonsyn_test)\n",
    "\n",
    "test_tensor = torch.utils.data.TensorDataset(x_test_tensor, ant_test_tensor)\n",
    "testing_data_set = torch.utils.data.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the neural network\n",
    "class ANT_TEST(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dims, out_dims):\n",
    "        super(ANT_TEST, self).__init__()\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding_layer = nn.Linear(in_dims, 100)\n",
    "        \n",
    "        #hidden layers\n",
    "        self.hidden_layer = nn.Linear(100, 32)\n",
    "        self.hidden_layer1 = nn.Linear(32, 16)\n",
    "        \n",
    "        #output layer\n",
    "        self.output_layer = nn.Linear(16,2)\n",
    "        #self.syn_branch = nn.Sequential(\n",
    "        #nn.Dropout(p = 0.3),\n",
    "        #nn.Linear(8,16),\n",
    "        #nn.Linear(16,32),\n",
    "        #nn.Linear(32,8),\n",
    "        #nn.Dropout(p = 0.3)\n",
    "        #nn.Softplus(),\n",
    "        #nn.Linear(8,2))\n",
    "        \n",
    "    def forward(self,x):\n",
    "       \n",
    "        #pass through embedding layer\n",
    "        out = self.embedding_layer(x)\n",
    "        \n",
    "        #pass through hidden layers\n",
    "        out = self.hidden_layer(out)\n",
    "        out = self.hidden_layer1(out)\n",
    "        \n",
    "        #pass to output layer\n",
    "        ant_out = self.output_layer(out)\n",
    "        \n",
    "        return ant_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_data_set, optimizer):\n",
    "    train_losses = []\n",
    "    ant_train_losses = []\n",
    "    train_epoch_loss = []\n",
    "    ant_train_epoch_loss = []\n",
    "    \n",
    "    ant_losses = []\n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    ant_criterion = nn.MSELoss()\n",
    "    \n",
    "    for features, labels in training_data_set:\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        ant_out = model(features) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        ant_loss = ant_criterion(ant_out, labels)\n",
    "        total_loss = ant_loss\n",
    "        \n",
    "        #BACKPROPAGATE LIKE A MF\n",
    "        torch.autograd.backward([ant_loss])\n",
    "        optimizer.step()\n",
    "        \n",
    "        #save loss for this batch\n",
    "        train_losses.append(total_loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        ant_train_losses.append(ant_loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    ant_train_epoch_loss.append(sum(ant_train_losses)/train_total)\n",
    "    \n",
    "    #update progress bar\n",
    "    print(f\"Total Epoch Training Loss is: {train_epoch_loss}\")\n",
    "    \n",
    "    return train_epoch_loss, ant_train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, testing_data_set, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    ant_criterion = nn.MSELoss()\n",
    "    #accuracy = #total number of correct predictions divided by the total number of predictions\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        ant_test_losses = []\n",
    "        #syn_test_acc_list = []\n",
    "        \n",
    "        test_total = 0\n",
    "\n",
    "        for inputs, labels in testing_data_set:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            ant_out = model(inputs)\n",
    "\n",
    "            # calculate loss per batch of testing data\n",
    "            ant_test_loss = ant_criterion(ant_out, labels)\n",
    "            \n",
    "            test_loss = ant_test_loss\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            ant_test_losses.append(ant_test_loss.item())\n",
    "            test_total += 1 \n",
    "            #ant_acc = accuracy(ant_out)\n",
    "            #ant_test_acc_list.append(ant_acc.item())\n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "        ant_test_epoch_loss = sum(ant_test_losses)/test_total\n",
    "        \n",
    "        #ant_epoch_acc = sum(ant_test_acc_list)/test_total\n",
    "\n",
    "        print(f\"Total Epoch Testing Loss is: {test_epoch_loss}\")\n",
    "        #print(f\"Epoch MAPE: Ant = {ant_epoch_acc}\")\n",
    "    \n",
    "    return test_epoch_loss, ant_test_epoch_loss, #ant_epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our beautiful NN model\n",
    "# takes in \n",
    "# predicts antonymy\n",
    "model = ANT_TEST(in_dims = 100, out_dims = 2).to(device)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Training Loss is: [238878.81310096153]\n",
      "Total Epoch Testing Loss is: 105366.56919642857\n",
      "Total Epoch Training Loss is: [66517.11470853366]\n",
      "Total Epoch Testing Loss is: 41123.38002232143\n",
      "Total Epoch Training Loss is: [39732.68825120192]\n",
      "Total Epoch Testing Loss is: 33680.001674107145\n",
      "Total Epoch Training Loss is: [32595.525540865383]\n",
      "Total Epoch Testing Loss is: 28960.910714285714\n",
      "Total Epoch Training Loss is: [29823.69200721154]\n",
      "Total Epoch Testing Loss is: 24535.818219866072\n",
      "Total Epoch Training Loss is: [28232.696927584133]\n",
      "Total Epoch Testing Loss is: 24217.968191964286\n",
      "Total Epoch Training Loss is: [26557.24894831731]\n",
      "Total Epoch Testing Loss is: 22796.87527901786\n",
      "Total Epoch Training Loss is: [25181.22539813702]\n",
      "Total Epoch Testing Loss is: 22260.993303571428\n",
      "Total Epoch Training Loss is: [24478.11564753606]\n",
      "Total Epoch Testing Loss is: 25885.741629464286\n",
      "Total Epoch Training Loss is: [23408.28361628606]\n",
      "Total Epoch Testing Loss is: 22642.818777901786\n",
      "Total Epoch Training Loss is: [22233.559795673078]\n",
      "Total Epoch Testing Loss is: 23567.541155133928\n",
      "Total Epoch Training Loss is: [22044.34795673077]\n",
      "Total Epoch Testing Loss is: 22598.8505859375\n",
      "Total Epoch Training Loss is: [21460.22355769231]\n",
      "Total Epoch Testing Loss is: 25341.884905133928\n",
      "Total Epoch Training Loss is: [20906.844951923078]\n",
      "Total Epoch Testing Loss is: 22178.74093191964\n",
      "Total Epoch Training Loss is: [20111.98035606971]\n",
      "Total Epoch Testing Loss is: 23240.805106026786\n",
      "Total Epoch Training Loss is: [20448.720928485578]\n",
      "Total Epoch Testing Loss is: 23826.253348214286\n",
      "Total Epoch Training Loss is: [20803.88153545673]\n",
      "Total Epoch Testing Loss is: 20760.2265625\n",
      "Total Epoch Training Loss is: [19144.534930889422]\n",
      "Total Epoch Testing Loss is: 22411.925920758928\n",
      "Total Epoch Training Loss is: [18990.369666466348]\n",
      "Total Epoch Testing Loss is: 21208.839983258928\n",
      "Total Epoch Training Loss is: [19966.160869891828]\n",
      "Total Epoch Testing Loss is: 21999.436941964286\n",
      "Total Epoch Training Loss is: [18912.35576923077]\n",
      "Total Epoch Testing Loss is: 20168.911551339286\n",
      "Total Epoch Training Loss is: [19229.16289813702]\n",
      "Total Epoch Testing Loss is: 21544.707170758928\n",
      "Total Epoch Training Loss is: [19256.73546424279]\n",
      "Total Epoch Testing Loss is: 23698.08858816964\n",
      "Total Epoch Training Loss is: [17685.66522686298]\n",
      "Total Epoch Testing Loss is: 23817.89306640625\n",
      "Total Epoch Training Loss is: [17322.80329777644]\n",
      "Total Epoch Testing Loss is: 20617.78292410714\n",
      "Total Epoch Training Loss is: [17533.871356670672]\n",
      "Total Epoch Testing Loss is: 24976.56905691964\n",
      "Total Epoch Training Loss is: [17742.47663762019]\n",
      "Total Epoch Testing Loss is: 20915.087472098214\n",
      "Total Epoch Training Loss is: [18399.954326923078]\n",
      "Total Epoch Testing Loss is: 22035.296316964286\n",
      "Total Epoch Training Loss is: [17557.11925330529]\n",
      "Total Epoch Testing Loss is: 23086.556640625\n",
      "Total Epoch Training Loss is: [17747.96698467548]\n",
      "Total Epoch Testing Loss is: 23094.770368303572\n",
      "Total Epoch Training Loss is: [18089.689302884617]\n",
      "Total Epoch Testing Loss is: 21131.945172991072\n",
      "Total Epoch Training Loss is: [16937.182391826922]\n",
      "Total Epoch Testing Loss is: 22942.09737723214\n",
      "Total Epoch Training Loss is: [16456.847412109375]\n",
      "Total Epoch Testing Loss is: 22350.573381696428\n",
      "Total Epoch Training Loss is: [16901.70860877404]\n",
      "Total Epoch Testing Loss is: 21789.511160714286\n",
      "Total Epoch Training Loss is: [16432.85723407452]\n",
      "Total Epoch Testing Loss is: 22856.878766741072\n",
      "Total Epoch Training Loss is: [16634.821570763223]\n",
      "Total Epoch Testing Loss is: 21781.587332589286\n",
      "Total Epoch Training Loss is: [16824.365215594953]\n",
      "Total Epoch Testing Loss is: 23166.248465401786\n",
      "Total Epoch Training Loss is: [16950.677095853367]\n",
      "Total Epoch Testing Loss is: 22585.99044363839\n",
      "Total Epoch Training Loss is: [17015.708477313703]\n",
      "Total Epoch Testing Loss is: 23348.02762276786\n",
      "Total Epoch Training Loss is: [17362.657752403848]\n",
      "Total Epoch Testing Loss is: 24242.639787946428\n",
      "Total Epoch Training Loss is: [16278.250112680289]\n",
      "Total Epoch Testing Loss is: 34135.28264508928\n",
      "Total Epoch Training Loss is: [16032.054274338941]\n",
      "Total Epoch Testing Loss is: 23487.828543526786\n",
      "Total Epoch Training Loss is: [16298.800180288461]\n",
      "Total Epoch Testing Loss is: 24190.94949776786\n",
      "Total Epoch Training Loss is: [17078.237041766828]\n",
      "Total Epoch Testing Loss is: 22297.33565848214\n",
      "Total Epoch Training Loss is: [17206.72907902644]\n",
      "Total Epoch Testing Loss is: 23929.06515066964\n",
      "Total Epoch Training Loss is: [16355.63592998798]\n",
      "Total Epoch Testing Loss is: 22589.92508370536\n",
      "Total Epoch Training Loss is: [16539.785775991586]\n",
      "Total Epoch Testing Loss is: 24716.800223214286\n",
      "Total Epoch Training Loss is: [16359.287823016826]\n",
      "Total Epoch Testing Loss is: 21005.552315848214\n",
      "Total Epoch Training Loss is: [15305.857309194711]\n",
      "Total Epoch Testing Loss is: 21520.18917410714\n",
      "Total Epoch Training Loss is: [15913.381272536059]\n",
      "Total Epoch Testing Loss is: 21001.10323660714\n",
      "Total Epoch Training Loss is: [15265.459698016826]\n",
      "Total Epoch Testing Loss is: 24279.175641741072\n",
      "Total Epoch Training Loss is: [15914.07256610577]\n",
      "Total Epoch Testing Loss is: 22493.996233258928\n",
      "Total Epoch Training Loss is: [15652.14473783053]\n",
      "Total Epoch Testing Loss is: 40177.316127232145\n",
      "Total Epoch Training Loss is: [16302.611778846154]\n",
      "Total Epoch Testing Loss is: 22920.673967633928\n",
      "Total Epoch Training Loss is: [16199.976975661059]\n",
      "Total Epoch Testing Loss is: 22222.273995535714\n",
      "Total Epoch Training Loss is: [15073.405442457934]\n",
      "Total Epoch Testing Loss is: 23080.97530691964\n",
      "Total Epoch Training Loss is: [15010.968111478365]\n",
      "Total Epoch Testing Loss is: 24089.546944754464\n",
      "Total Epoch Training Loss is: [15058.607666015625]\n",
      "Total Epoch Testing Loss is: 22395.000837053572\n",
      "Total Epoch Training Loss is: [15586.552377554086]\n",
      "Total Epoch Testing Loss is: 22006.03857421875\n",
      "Total Epoch Training Loss is: [15694.445875901441]\n",
      "Total Epoch Testing Loss is: 22248.14481026786\n",
      "Total Epoch Training Loss is: [15452.018930288461]\n",
      "Total Epoch Testing Loss is: 21133.271833147322\n",
      "Total Epoch Training Loss is: [15213.46905048077]\n",
      "Total Epoch Testing Loss is: 22642.918317522322\n",
      "Total Epoch Training Loss is: [15936.001802884615]\n",
      "Total Epoch Testing Loss is: 22585.636579241072\n",
      "Total Epoch Training Loss is: [15537.73105093149]\n",
      "Total Epoch Testing Loss is: 22349.68973214286\n",
      "Total Epoch Training Loss is: [15511.33340219351]\n",
      "Total Epoch Testing Loss is: 22906.150948660714\n",
      "Total Epoch Training Loss is: [15261.444073016826]\n",
      "Total Epoch Testing Loss is: 23042.001116071428\n",
      "Total Epoch Training Loss is: [15735.01806640625]\n",
      "Total Epoch Testing Loss is: 22583.24734933036\n",
      "Total Epoch Training Loss is: [14917.21112530048]\n",
      "Total Epoch Testing Loss is: 22485.4736328125\n",
      "Total Epoch Training Loss is: [15250.140737680289]\n",
      "Total Epoch Testing Loss is: 25913.5498046875\n",
      "Total Epoch Training Loss is: [15682.076096754809]\n",
      "Total Epoch Testing Loss is: 22010.826311383928\n",
      "Total Epoch Training Loss is: [16670.649639423078]\n",
      "Total Epoch Testing Loss is: 22270.873604910714\n",
      "Total Epoch Training Loss is: [15507.317326472355]\n",
      "Total Epoch Testing Loss is: 22702.93094308036\n",
      "Total Epoch Training Loss is: [14656.211876502404]\n",
      "Total Epoch Testing Loss is: 36855.18247767857\n",
      "Total Epoch Training Loss is: [14577.035400390625]\n",
      "Total Epoch Testing Loss is: 21274.102190290178\n",
      "Total Epoch Training Loss is: [14887.893892728365]\n",
      "Total Epoch Testing Loss is: 24200.997488839286\n",
      "Total Epoch Training Loss is: [15367.621920072115]\n",
      "Total Epoch Testing Loss is: 22769.765485491072\n",
      "Total Epoch Training Loss is: [15010.696458082934]\n",
      "Total Epoch Testing Loss is: 21941.6572265625\n",
      "Total Epoch Training Loss is: [15276.84384390024]\n",
      "Total Epoch Testing Loss is: 22820.321219308036\n",
      "Total Epoch Training Loss is: [15107.519512469951]\n",
      "Total Epoch Testing Loss is: 24106.1611328125\n",
      "Total Epoch Training Loss is: [15085.301870492789]\n",
      "Total Epoch Testing Loss is: 23595.517159598214\n",
      "Total Epoch Training Loss is: [15300.225736177885]\n",
      "Total Epoch Testing Loss is: 23501.427734375\n",
      "Total Epoch Training Loss is: [15951.34542142428]\n",
      "Total Epoch Testing Loss is: 23601.33426339286\n",
      "Total Epoch Training Loss is: [15983.513746995191]\n",
      "Total Epoch Testing Loss is: 23740.62276785714\n",
      "Total Epoch Training Loss is: [14742.2373046875]\n",
      "Total Epoch Testing Loss is: 22943.43233816964\n",
      "Total Epoch Training Loss is: [16901.14509465144]\n",
      "Total Epoch Testing Loss is: 39387.625\n",
      "Total Epoch Training Loss is: [15165.92626953125]\n",
      "Total Epoch Testing Loss is: 22372.41162109375\n",
      "Total Epoch Training Loss is: [15182.84393780048]\n",
      "Total Epoch Testing Loss is: 21302.802734375\n",
      "Total Epoch Training Loss is: [14084.09023813101]\n",
      "Total Epoch Testing Loss is: 21080.929547991072\n",
      "Total Epoch Training Loss is: [15334.33514873798]\n",
      "Total Epoch Testing Loss is: 22983.81417410714\n",
      "Total Epoch Training Loss is: [15855.593487079326]\n",
      "Total Epoch Testing Loss is: 21253.138671875\n",
      "Total Epoch Training Loss is: [16128.263746995191]\n",
      "Total Epoch Testing Loss is: 26422.705496651786\n",
      "Total Epoch Training Loss is: [15917.802114633414]\n",
      "Total Epoch Testing Loss is: 21390.168108258928\n",
      "Total Epoch Training Loss is: [15152.947115384615]\n",
      "Total Epoch Testing Loss is: 22758.098074776786\n",
      "Total Epoch Training Loss is: [15273.137451171875]\n",
      "Total Epoch Testing Loss is: 22873.2177734375\n",
      "Total Epoch Training Loss is: [14395.741849459135]\n",
      "Total Epoch Testing Loss is: 22333.19070870536\n",
      "Total Epoch Training Loss is: [14643.129770132211]\n",
      "Total Epoch Testing Loss is: 22019.95486886161\n",
      "Total Epoch Training Loss is: [14777.683668870191]\n",
      "Total Epoch Testing Loss is: 22061.379743303572\n",
      "Total Epoch Training Loss is: [15062.037766676684]\n",
      "Total Epoch Testing Loss is: 22978.80517578125\n",
      "Total Epoch Training Loss is: [14743.31278170072]\n",
      "Total Epoch Testing Loss is: 21321.351702008928\n",
      "Total Epoch Training Loss is: [15227.76327749399]\n",
      "Total Epoch Testing Loss is: 22354.807686941964\n"
     ]
    }
   ],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "ant_train_epoch_losses = []\n",
    "\n",
    "test_epoch_losses = []\n",
    "ant_test_epoch_losses = []\n",
    "\n",
    "ant_test_epoch_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, ant_train_epoch_loss  = train_model(model = model, training_data_set = training_data_set, optimizer = optimizer)\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    ant_train_epoch_losses.append(ant_train_epoch_loss)\n",
    "   \n",
    "    test_epoch_loss, ant_test_epoch_loss = eval_model(model = model, testing_data_set = testing_data_set, optimizer = optimizer)\n",
    "    #ant_epoch_acc\n",
    "    \n",
    "    test_epoch_losses.append(test_epoch_loss)\n",
    "    ant_test_epoch_losses.append(ant_test_epoch_loss)\n",
    "    \n",
    "    #pce_test_epoch_accuracies.append(pce_epoch_acc)\n",
    "    #voc_test_epoch_accuracies.append(voc_epoch_acc)\n",
    "    #jsc_test_epoch_accuracies.append(jsc_epoch_acc)\n",
    "    #ff_test_epoch_accuracies.append(ff_epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "epochs = np.arange(1, (num_epochs+1), 1)\n",
    "\n",
    "plt.plot(epochs, train_epoch_losses, c = 'k', label = 'training error')\n",
    "plt.plot(epochs, test_epoch_losses, c = 'r', label = 'testing error')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.title(\"Total Training & Testing Error\")\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (8,6))\n",
    "# plt.plot(epochs, train_epoch_accuracy, c = 'k', label = 'training accuracy')\n",
    "#plt.plot(epochs, pce_test_epoch_accuracies, c = 'k', label = 'pce accuracy')\n",
    "#plt.plot(epochs, voc_test_epoch_accuracies, c = 'r', label = 'voc accuracy')\n",
    "#plt.plot(epochs, jsc_test_epoch_accuracies, c = 'g', label = 'jsc accuracy')\n",
    "#plt.plot(epochs, ff_test_epoch_accuracies, c = 'b', label = 'ff accuracy')\n",
    "#plt.legend(loc = 'lower right')\n",
    "#plt.title(\"Branch Mean Absolute Percent Error\")\n",
    "#ax.set_xlabel('Epoch')\n",
    "#ax.set_ylabel('MAPE')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
